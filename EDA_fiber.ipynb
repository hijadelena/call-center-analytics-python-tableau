{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqi/Gr9aGqsNVwzAFfJIMs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hijadelena/call-center-analytics-python-tableau/blob/main/EDA_fiber.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üß©Dashboard de An√°lisis de Llamadas Repetidas al Servicio de Atenci√≥n al Cliente**"
      ],
      "metadata": {
        "id": "5gPCxM_xnrMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Limpieza y carga de datos, los pasos que hacemos**\n",
        "\n",
        "Cargar los datasets desde Google Sheets\n",
        "\n",
        "Estandarizar nombres de columnas\n",
        "\n",
        "Eliminar duplicados y valores nulos\n",
        "\n",
        "Convertir columnas de fecha\n",
        "\n",
        "Verificar tipos de datos\n",
        "\n",
        "A√±adir columnas calculadas √∫tiles para Tableau\n",
        "\n"
      ],
      "metadata": {
        "id": "qUhAwOGRn1MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "def clean_google_fiber_data():\n",
        "    \"\"\"\n",
        "    Limpieza mejorada de datos de Google Fiber con manejo robusto de nulos\n",
        "    y agregaci√≥n de fechas duplicadas\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Cargar datos desde Google Sheets\n",
        "    print(\"üìä Cargando datos de Google Sheets...\")\n",
        "    url_market1 = 'https://docs.google.com/spreadsheets/d/1T7irtn0ay9MfuhG_6y2jEbeT-zLuEQTbv1qNxkpBaTE/export?format=csv'\n",
        "    url_market2 = 'https://docs.google.com/spreadsheets/d/1RE_MAKPt0JfWijbYCR32XFra9OGlmS2tLMxNs-X2NSU/export?format=csv'\n",
        "\n",
        "    try:\n",
        "        df1 = pd.read_csv(url_market1)\n",
        "        df2 = pd.read_csv(url_market2)\n",
        "        print(f\"‚úÖ Market 1: {len(df1)} registros\")\n",
        "        print(f\"‚úÖ Market 2: {len(df2)} registros\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error cargando datos: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 2. Estandarizar nombres de columnas\n",
        "    print(\"\\nüîß Estandarizando columnas...\")\n",
        "    df1.columns = df1.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "    df2.columns = df2.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "    # 3. Verificar estructura com√∫n\n",
        "    if set(df1.columns) != set(df2.columns):\n",
        "        print(\"‚ö†Ô∏è Diferencias en columnas:\")\n",
        "        print(f\"Solo en Market 1: {set(df1.columns) - set(df2.columns)}\")\n",
        "        print(f\"Solo en Market 2: {set(df2.columns) - set(df1.columns)}\")\n",
        "        # Tomar columnas comunes\n",
        "        common_cols = list(set(df1.columns) & set(df2.columns))\n",
        "        df1 = df1[common_cols]\n",
        "        df2 = df2[common_cols]\n",
        "\n",
        "    # 4. Unir dataframes\n",
        "    print(\"üîó Uniendo datasets...\")\n",
        "    df = pd.concat([df1, df2], ignore_index=True)\n",
        "    print(f\"Dataset combinado: {len(df)} registros\")\n",
        "\n",
        "    # 5. An√°lisis inicial de nulos\n",
        "    print(\"\\nüìã An√°lisis inicial de valores nulos:\")\n",
        "    null_counts = df.isnull().sum()\n",
        "    null_percentages = (df.isnull().sum() / len(df)) * 100\n",
        "    null_summary = pd.DataFrame({\n",
        "        'Nulos': null_counts,\n",
        "        'Porcentaje': null_percentages\n",
        "    }).sort_values('Porcentaje', ascending=False)\n",
        "    print(null_summary[null_summary['Nulos'] > 0])\n",
        "\n",
        "    # 6. Limpiar y convertir fecha (initial attempt)\n",
        "    print(\"\\nüìÖ Procesando fechas...\")\n",
        "    if 'date_created' in df.columns:\n",
        "        # Probar m√∫ltiples formatos de fecha\n",
        "        date_formats = ['%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%Y-%m-%d %H:%M:%S']\n",
        "        df['date_created_clean'] = None\n",
        "\n",
        "        for fmt in date_formats:\n",
        "            mask = df['date_created_clean'].isnull()\n",
        "            try:\n",
        "                df.loc[mask, 'date_created_clean'] = pd.to_datetime(\n",
        "                    df.loc[mask, 'date_created'],\n",
        "                    format=fmt,\n",
        "                    errors='coerce'\n",
        "                )\n",
        "                converted = mask.sum() - df['date_created_clean'].isnull().sum()\n",
        "                if converted > 0:\n",
        "                    print(f\"‚úÖ Convertidas {converted} fechas con formato {fmt}\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Eliminar registros con fechas inv√°lidas\n",
        "        invalid_dates = df['date_created_clean'].isnull().sum()\n",
        "        if invalid_dates > 0:\n",
        "            print(f\"‚ö†Ô∏è Eliminando {invalid_dates} registros con fechas inv√°lidas\")\n",
        "            df = df.dropna(subset=['date_created_clean'])\n",
        "\n",
        "        df['date_created'] = df['date_created_clean']\n",
        "        df = df.drop('date_created_clean', axis=1)\n",
        "        # Ensure date_created is datetime after initial cleaning\n",
        "        df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "\n",
        "\n",
        "    # 7. Identificar y limpiar columnas de contactos\n",
        "    print(\"\\n‚òéÔ∏è Procesando columnas de contactos...\")\n",
        "    contact_columns = [col for col in df.columns if 'contacts_' in col]\n",
        "    print(f\"Columnas de contactos encontradas: {contact_columns}\")\n",
        "\n",
        "    # Convertir a num√©rico y llenar nulos con 0\n",
        "    for col in contact_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # 8. Limpiar columnas categ√≥ricas\n",
        "    print(\"\\nüè∑Ô∏è Limpiando columnas categ√≥ricas...\")\n",
        "    categorical_columns = []\n",
        "\n",
        "    # Identificar columnas de tipo/mercado\n",
        "    for col in df.columns:\n",
        "        if any(keyword in col.lower() for keyword in ['type', 'market', 'new_']):\n",
        "            categorical_columns.append(col)\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        if col in df.columns:\n",
        "            # Limpiar espacios y valores vac√≠os\n",
        "            df[col] = df[col].astype(str).str.strip()\n",
        "            df[col] = df[col].replace(['', 'nan', 'None', 'null'], np.nan)\n",
        "\n",
        "            # Mostrar valores √∫nicos para revisi√≥n\n",
        "            unique_values = df[col].value_counts(dropna=False)\n",
        "            print(f\"\\n{col} - Valores √∫nicos:\")\n",
        "            print(unique_values.head(10))\n",
        "\n",
        "    # 9. Manejar fechas duplicadas - AGREGACI√ìN\n",
        "    print(\"\\nüìä Manejando fechas duplicadas...\")\n",
        "\n",
        "    # Definir columnas de agrupaci√≥n (excluyendo contactos)\n",
        "    group_columns = ['date_created']\n",
        "    if 'new_market' in df.columns:\n",
        "        group_columns.append('new_market')\n",
        "    if 'new_type' in df.columns:\n",
        "        group_columns.append('new_type')\n",
        "\n",
        "    # Ensure group_columns are in df.columns before grouping\n",
        "    group_columns = [col for col in group_columns if col in df.columns]\n",
        "\n",
        "    # Convert date_created to datetime before checking for duplicates\n",
        "    df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "\n",
        "\n",
        "    # Identify duplicates before aggregation\n",
        "    duplicates_before = df.duplicated(subset=group_columns).sum()\n",
        "    print(f\"Registros duplicados por fecha/mercado/tipo: {duplicates_before}\")\n",
        "\n",
        "    if duplicates_before > 0:\n",
        "        print(\"üîÑ Agregando datos por fecha/mercado/tipo...\")\n",
        "\n",
        "        # Agregaci√≥n: sumar columnas de contactos\n",
        "        agg_dict = {}\n",
        "        for col in contact_columns:\n",
        "            agg_dict[col] = 'sum'\n",
        "\n",
        "        # Maintain first occurrence for other columns, excluding date_created\n",
        "        for col in df.columns:\n",
        "            if col not in contact_columns + group_columns and col != 'date_created':\n",
        "                agg_dict[col] = 'first'\n",
        "\n",
        "        df_aggregated = df.groupby(group_columns, as_index=False).agg(agg_dict)\n",
        "        print(f\"‚úÖ Datos agregados: {len(df)} ‚Üí {len(df_aggregated)} registros\")\n",
        "        df = df_aggregated\n",
        "        # Ensure date_created is datetime after aggregation\n",
        "        df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "\n",
        "\n",
        "    # 10. Renombrar columnas para claridad\n",
        "    print(\"\\nüè∑Ô∏è Renombrando columnas...\")\n",
        "    rename_mapping = {\n",
        "        'new_type': 'problem_type',\n",
        "        'new_market': 'market'\n",
        "    }\n",
        "    df = df.rename(columns=rename_mapping)\n",
        "\n",
        "    # 11. Calcular m√©tricas clave\n",
        "    print(\"\\nüìà Calculando m√©tricas...\")\n",
        "\n",
        "    # Identificar columnas de llamadas repetidas\n",
        "    repeat_columns = [col for col in contact_columns if '_n_' in col and col != 'contacts_n']\n",
        "\n",
        "    # Calcular totales\n",
        "    df['total_repeats'] = df[repeat_columns].sum(axis=1)\n",
        "    df['total_calls'] = df['contacts_n'] + df['total_repeats']\n",
        "    df['resolved_first_call'] = (df['total_repeats'] == 0).astype(int)\n",
        "    df['repeat_call_flag'] = (df['total_repeats'] > 0).astype(int)\n",
        "\n",
        "    # Calcular tasa de resoluci√≥n en primera llamada\n",
        "    if len(df) > 0:\n",
        "        fcr_rate = (df['resolved_first_call'].sum() / len(df)) * 100\n",
        "        repeat_rate = (df['repeat_call_flag'].sum() / len(df)) * 100\n",
        "        print(f\"üìä FCR Rate: {fcr_rate:.2f}%\")\n",
        "        print(f\"üìä Repeat Call Rate: {repeat_rate:.2f}%\")\n",
        "\n",
        "    # 12. Crear variables temporales para an√°lisis\n",
        "    print(\"\\nüìÖ Creando variables temporales...\")\n",
        "    # Ensure date_created is datetime before using .dt accessor\n",
        "    df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "    df['year'] = df['date_created'].dt.year\n",
        "    df['month'] = df['date_created'].dt.month\n",
        "    df['week'] = df['date_created'].dt.isocalendar().week\n",
        "    df['day_of_week'] = df['date_created'].dt.day_name()\n",
        "    df['date_only'] = df['date_created'].dt.date\n",
        "\n",
        "    # Per√≠odos para agrupaci√≥n\n",
        "    df['year_month'] = df['date_created'].dt.to_period('M').astype(str)\n",
        "    df['year_week'] = df['date_created'].dt.to_period('W').astype(str)\n",
        "\n",
        "    # 13. Validaci√≥n final\n",
        "    print(\"\\n‚úÖ Validaci√≥n final...\")\n",
        "    print(f\"Registros finales: {len(df)}\")\n",
        "    print(f\"Rango de fechas: {df['date_created'].min()} - {df['date_created'].max()}\")\n",
        "    print(f\"Nulos restantes por columna:\")\n",
        "\n",
        "    final_nulls = df.isnull().sum()\n",
        "    if final_nulls.sum() > 0:\n",
        "        print(final_nulls[final_nulls > 0])\n",
        "    else:\n",
        "        print(\"Sin valores nulos ‚úÖ\")\n",
        "\n",
        "    # 14. Estad√≠sticas descriptivas\n",
        "    print(\"\\nüìä Estad√≠sticas descriptivas:\")\n",
        "    numeric_cols = ['contacts_n', 'total_repeats', 'total_calls']\n",
        "    print(df[numeric_cols].describe())\n",
        "\n",
        "    # 15. Exportar datos limpios\n",
        "    output_file = 'google_fiber_clean.csv'\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nüíæ Datos exportados a: {output_file}\")\n",
        "\n",
        "    # 16. Generar reporte de calidad\n",
        "    generate_data_quality_report(df, output_file.replace('.csv', '_quality_report.txt'))\n",
        "\n",
        "    return df\n",
        "\n",
        "def generate_data_quality_report(df, filename):\n",
        "    \"\"\"Generar reporte de calidad de datos\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"GOOGLE FIBER - REPORTE DE CALIDAD DE DATOS\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Fecha de generaci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "        f.write(\"1. RESUMEN GENERAL\\n\")\n",
        "        f.write(\"-\" * 20 + \"\\n\")\n",
        "        f.write(f\"Total de registros: {len(df):,}\\n\")\n",
        "        f.write(f\"Total de columnas: {len(df.columns)}\\n\")\n",
        "        f.write(f\"Rango de fechas: {df['date_created'].min()} - {df['date_created'].max()}\\n\")\n",
        "        f.write(f\"Per√≠odo total: {(df['date_created'].max() - df['date_created'].min()).days} d√≠as\\n\\n\")\n",
        "\n",
        "        f.write(\"2. DISTRIBUCI√ìN POR MERCADO\\n\")\n",
        "        f.write(\"-\" * 30 + \"\\n\")\n",
        "        if 'market' in df.columns:\n",
        "            market_dist = df['market'].value_counts()\n",
        "            for market, count in market_dist.items():\n",
        "                f.write(f\"{market}: {count:,} registros ({count/len(df)*100:.1f}%)\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"3. DISTRIBUCI√ìN POR TIPO DE PROBLEMA\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        if 'problem_type' in df.columns:\n",
        "            type_dist = df['problem_type'].value_counts()\n",
        "            for ptype, count in type_dist.items():\n",
        "                f.write(f\"{ptype}: {count:,} registros ({count/len(df)*100:.1f}%)\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"4. M√âTRICAS DE RENDIMIENTO\\n\")\n",
        "        f.write(\"-\" * 30 + \"\\n\")\n",
        "        fcr_rate = (df['resolved_first_call'].sum() / len(df)) * 100\n",
        "        repeat_rate = (df['repeat_call_flag'].sum() / len(df)) * 100\n",
        "        avg_calls = df['total_calls'].mean()\n",
        "\n",
        "        f.write(f\"Tasa de resoluci√≥n en primera llamada (FCR): {fcr_rate:.2f}%\\n\")\n",
        "        f.write(f\"Tasa de llamadas repetidas: {repeat_rate:.2f}%\\n\")\n",
        "        f.write(f\"Promedio de llamadas por caso: {avg_calls:.2f}\\n\\n\")\n",
        "\n",
        "        f.write(\"5. COLUMNAS DISPONIBLES PARA TABLEAU\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for col in sorted(df.columns):\n",
        "            dtype = str(df[col].dtype)\n",
        "            nulls = df[col].isnull().sum()\n",
        "            f.write(f\"{col} ({dtype}) - Nulos: {nulls}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Iniciando limpieza de datos Google Fiber...\")\n",
        "    clean_data = clean_google_fiber_data()\n",
        "    print(\"\\nüéâ Proceso completado!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piu_9Zm7NNZe",
        "outputId": "633f0bc4-8ffa-41a1-901e-537be19e8a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Iniciando limpieza de datos Google Fiber...\n",
            "üìä Cargando datos de Google Sheets...\n",
            "‚úÖ Market 1: 450 registros\n",
            "‚úÖ Market 2: 450 registros\n",
            "\n",
            "üîß Estandarizando columnas...\n",
            "üîó Uniendo datasets...\n",
            "Dataset combinado: 900 registros\n",
            "\n",
            "üìã An√°lisis inicial de valores nulos:\n",
            "              Nulos  Porcentaje\n",
            "contacts_n_6    445   49.444444\n",
            "contacts_n_5    439   48.777778\n",
            "contacts_n_7    435   48.333333\n",
            "contacts_n_4    432   48.000000\n",
            "contacts_n_3    404   44.888889\n",
            "contacts_n_2    374   41.555556\n",
            "contacts_n_1    328   36.444444\n",
            "contacts_n      132   14.666667\n",
            "\n",
            "üìÖ Procesando fechas...\n",
            "‚úÖ Convertidas 900 fechas con formato %Y-%m-%d\n",
            "\n",
            "‚òéÔ∏è Procesando columnas de contactos...\n",
            "Columnas de contactos encontradas: ['contacts_n', 'contacts_n_1', 'contacts_n_2', 'contacts_n_3', 'contacts_n_4', 'contacts_n_5', 'contacts_n_6', 'contacts_n_7']\n",
            "\n",
            "üè∑Ô∏è Limpiando columnas categ√≥ricas...\n",
            "\n",
            "new_type - Valores √∫nicos:\n",
            "new_type\n",
            "type_5    180\n",
            "type_1    180\n",
            "type_2    180\n",
            "type_4    180\n",
            "type_3    180\n",
            "Name: count, dtype: int64\n",
            "\n",
            "new_market - Valores √∫nicos:\n",
            "new_market\n",
            "market_1    450\n",
            "market_2    450\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üìä Manejando fechas duplicadas...\n",
            "Registros duplicados por fecha/mercado/tipo: 0\n",
            "\n",
            "üè∑Ô∏è Renombrando columnas...\n",
            "\n",
            "üìà Calculando m√©tricas...\n",
            "üìä FCR Rate: 28.00%\n",
            "üìä Repeat Call Rate: 72.00%\n",
            "\n",
            "üìÖ Creando variables temporales...\n",
            "\n",
            "‚úÖ Validaci√≥n final...\n",
            "Registros finales: 900\n",
            "Rango de fechas: 2022-01-01 00:00:00 - 2022-03-31 00:00:00\n",
            "Nulos restantes por columna:\n",
            "Sin valores nulos ‚úÖ\n",
            "\n",
            "üìä Estad√≠sticas descriptivas:\n",
            "       contacts_n  total_repeats  total_calls\n",
            "count  900.000000     900.000000   900.000000\n",
            "mean    55.246667      14.926667    70.173333\n",
            "std     95.214851      25.155869   119.002823\n",
            "min      0.000000       0.000000     0.000000\n",
            "25%      2.000000       0.000000     2.000000\n",
            "50%     15.000000       3.000000    19.000000\n",
            "75%     33.000000      11.000000    42.250000\n",
            "max    599.000000     119.000000   683.000000\n",
            "\n",
            "üíæ Datos exportados a: google_fiber_clean.csv\n",
            "\n",
            "üéâ Proceso completado!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----CORRECCION DE METRICAS----\n",
        "se encontro que el fcr y otras metricas tenian, Error en el c√≥digo Python:\n",
        "pythondf['resolved_first_call'] = (df['total_repeats'] == 0).astype(int)\n",
        "Esta l√≥gica considera que si total_repeats == 0, entonces se resolvi√≥ en primera llamada. Pero esto es incorrecto porque:\n",
        "\n",
        "Un registro puede tener contacts_n = 0 (sin primera llamada) y total_repeats = 0\n",
        "Esto marcar√≠a err√≥neamente como \"resuelto en primera llamada\"\\"
      ],
      "metadata": {
        "id": "CZLR1Dt-69ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "def complete_corrected_analysis(df):\n",
        "    \"\"\"\n",
        "    An√°lisis completo con TODAS las m√©tricas corregidas + an√°lisis de patrones\n",
        "    \"\"\"\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. CORRECCI√ìN COMPLETA DE M√âTRICAS\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"üîß CORRIGIENDO TODAS LAS M√âTRICAS...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Identificar columnas de contactos\n",
        "    contact_columns = [col for col in df.columns if 'contacts_n' in col]\n",
        "    repeat_columns = [col for col in contact_columns if 'contacts_n_' in col]\n",
        "\n",
        "    print(f\"üìû Columna primera llamada: contacts_n\")\n",
        "    print(f\"üîÅ Columnas repetidas: {repeat_columns}\")\n",
        "\n",
        "    # Calcular totales CORRECTOS\n",
        "    df['first_calls'] = df['contacts_n'].fillna(0)\n",
        "    df['repeat_n1'] = df['contacts_n_1'].fillna(0) if 'contacts_n_1' in df.columns else 0\n",
        "    df['repeat_n2'] = df['contacts_n_2'].fillna(0) if 'contacts_n_2' in df.columns else 0\n",
        "    df['repeat_n3'] = df['contacts_n_3'].fillna(0) if 'contacts_n_3' in df.columns else 0\n",
        "    df['repeat_n4'] = df['contacts_n_4'].fillna(0) if 'contacts_n_4' in df.columns else 0\n",
        "    df['repeat_n5'] = df['contacts_n_5'].fillna(0) if 'contacts_n_5' in df.columns else 0\n",
        "    df['repeat_n6'] = df['contacts_n_6'].fillna(0) if 'contacts_n_6' in df.columns else 0\n",
        "    df['repeat_n7'] = df['contacts_n_7'].fillna(0) if 'contacts_n_7' in df.columns else 0\n",
        "\n",
        "    # Total repetidas y total llamadas\n",
        "    df['total_repeats'] = df[repeat_columns].sum(axis=1)\n",
        "    df['total_calls'] = df['first_calls'] + df['total_repeats']\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. M√âTRICAS CORREGIDAS (COMO TABLEAU)\n",
        "    # ==========================================\n",
        "\n",
        "    # Totales agregados\n",
        "    total_first_calls = df['first_calls'].sum()\n",
        "    total_repeat_calls = df['total_repeats'].sum()\n",
        "    total_all_calls = df['total_calls'].sum()\n",
        "\n",
        "    # FCR y Repeat Rate (como Tableau)\n",
        "    fcr_rate = (1 - (total_repeat_calls / total_all_calls)) * 100 if total_all_calls > 0 else 0\n",
        "    repeat_rate = (total_repeat_calls / total_all_calls) * 100 if total_all_calls > 0 else 0\n",
        "\n",
        "    # Casos √∫nicos vs llamadas totales\n",
        "    total_cases = len(df)\n",
        "    cases_no_repeat = (df['total_repeats'] == 0).sum()\n",
        "    cases_with_repeat = (df['total_repeats'] > 0).sum()\n",
        "\n",
        "    print(\"\\nüìä M√âTRICAS PRINCIPALES (CORREGIDAS):\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Total de llamadas: {total_all_calls:,}\")\n",
        "    print(f\"‚îú‚îÄ Primera llamada: {total_first_calls:,}\")\n",
        "    print(f\"‚îî‚îÄ Llamadas repetidas: {total_repeat_calls:,}\")\n",
        "    print(f\"FCR Rate: {fcr_rate:.1f}%\")\n",
        "    print(f\"Repeat Rate: {repeat_rate:.1f}%\")\n",
        "    print(f\"Casos totales: {total_cases:,}\")\n",
        "    print(f\"‚îú‚îÄ Sin repetici√≥n: {cases_no_repeat:,} ({cases_no_repeat/total_cases*100:.1f}%)\")\n",
        "    print(f\"‚îî‚îÄ Con repetici√≥n: {cases_with_repeat:,} ({cases_with_repeat/total_cases*100:.1f}%)\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. AN√ÅLISIS DE ESCALAMIENTO (N1 ‚Üí N7)\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"\\nüîç AN√ÅLISIS DE ESCALAMIENTO POR CONTACTO:\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    escalamiento_data = []\n",
        "    for i, col in enumerate(repeat_columns, 1):\n",
        "        if col in df.columns:\n",
        "            total_contacto = df[col].sum()\n",
        "            pct_del_total = (total_contacto / total_repeat_calls * 100) if total_repeat_calls > 0 else 0\n",
        "            escalamiento_data.append({\n",
        "                'Contacto': f'N{i}',\n",
        "                'Llamadas': total_contacto,\n",
        "                'Porcentaje': pct_del_total\n",
        "            })\n",
        "            print(f\"Contacto N{i}: {total_contacto:,} llamadas ({pct_del_total:.1f}%)\")\n",
        "\n",
        "    escalamiento_df = pd.DataFrame(escalamiento_data)\n",
        "\n",
        "    # Detectar picos de escalamiento\n",
        "    if len(escalamiento_df) > 0:\n",
        "        max_contacto = escalamiento_df.loc[escalamiento_df['Llamadas'].idxmax()]\n",
        "        print(f\"\\nüéØ Pico m√°ximo en: {max_contacto['Contacto']} con {max_contacto['Llamadas']:,} llamadas\")\n",
        "\n",
        "        # Identificar aumentos inusuales (como el N6 que mencionas)\n",
        "        aumentos = []\n",
        "        for i in range(1, len(escalamiento_df)):\n",
        "            anterior = escalamiento_df.iloc[i-1]['Llamadas']\n",
        "            actual = escalamiento_df.iloc[i]['Llamadas']\n",
        "            if actual > anterior:\n",
        "                aumento_pct = ((actual - anterior) / anterior * 100) if anterior > 0 else 0\n",
        "                aumentos.append(f\"N{i+1} (+{aumento_pct:.1f}% vs N{i})\")\n",
        "\n",
        "        if aumentos:\n",
        "            print(f\"‚ö†Ô∏è  Aumentos detectados en: {', '.join(aumentos)}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. AN√ÅLISIS POR TIPO DE PROBLEMA\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"\\nüìã AN√ÅLISIS POR TIPO DE PROBLEMA:\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    # Mapeo de tipos (seg√∫n tu informaci√≥n)\n",
        "    type_mapping = {\n",
        "        'type_1': 'Account Manager',\n",
        "        'type_2': 'Technical',\n",
        "        'type_3': 'Scheduling',\n",
        "        'type_4': 'Construction',\n",
        "        'type_5': 'Internet/WiFi'\n",
        "    }\n",
        "\n",
        "    if 'problem_type' in df.columns:\n",
        "        # Aplicar mapeo si es necesario\n",
        "        df['problem_type_label'] = df['problem_type'].map(type_mapping).fillna(df['problem_type'])\n",
        "\n",
        "        problem_analysis = df.groupby('problem_type_label').agg({\n",
        "            'first_calls': 'sum',\n",
        "            'total_repeats': 'sum',\n",
        "            'total_calls': 'sum',\n",
        "            'problem_type': 'count'  # casos\n",
        "        }).rename(columns={'problem_type': 'casos'})\n",
        "\n",
        "        problem_analysis['fcr_pct'] = (1 - (problem_analysis['total_repeats'] / problem_analysis['total_calls'])) * 100\n",
        "        problem_analysis['repeat_rate'] = (problem_analysis['total_repeats'] / problem_analysis['total_calls']) * 100\n",
        "        problem_analysis['avg_calls_per_case'] = problem_analysis['total_calls'] / problem_analysis['casos']\n",
        "\n",
        "        # Ordenar por problemas m√°s problem√°ticos\n",
        "        problem_analysis = problem_analysis.sort_values('repeat_rate', ascending=False)\n",
        "\n",
        "        print(\"\\nRanking por Mayor Tasa de Repetici√≥n:\")\n",
        "        for idx, row in problem_analysis.iterrows():\n",
        "            print(f\"{idx}:\")\n",
        "            print(f\"  ‚Ä¢ Llamadas repetidas: {row['total_repeats']:,} ({row['repeat_rate']:.1f}%)\")\n",
        "            print(f\"  ‚Ä¢ FCR: {row['fcr_pct']:.1f}%\")\n",
        "            print(f\"  ‚Ä¢ Promedio llamadas/caso: {row['avg_calls_per_case']:.1f}\")\n",
        "\n",
        "        # An√°lisis de escalamiento por tipo\n",
        "        print(f\"\\nüîç ESCALAMIENTO POR TIPO DE PROBLEMA:\")\n",
        "        escalamiento_por_tipo = []\n",
        "\n",
        "        for problema in df['problem_type_label'].unique():\n",
        "            if pd.notna(problema):\n",
        "                subset = df[df['problem_type_label'] == problema]\n",
        "                escalamiento_tipo = {}\n",
        "\n",
        "                for i, col in enumerate(repeat_columns, 1):\n",
        "                    if col in subset.columns:\n",
        "                        escalamiento_tipo[f'N{i}'] = subset[col].sum()\n",
        "\n",
        "                # Buscar picos en N6 espec√≠ficamente\n",
        "                if 'N6' in escalamiento_tipo and len(escalamiento_tipo) > 5:\n",
        "                    n5_val = escalamiento_tipo.get('N5', 0)\n",
        "                    n6_val = escalamiento_tipo.get('N6', 0)\n",
        "                    n7_val = escalamiento_tipo.get('N7', 0)\n",
        "\n",
        "                    if n6_val > n5_val and n6_val > n7_val and n6_val > 0:\n",
        "                        pct_aumento = ((n6_val - n5_val) / n5_val * 100) if n5_val > 0 else 0\n",
        "                        print(f\"  ‚ö†Ô∏è  {problema}: Pico en N6 ({n6_val:,} llamadas, +{pct_aumento:.1f}% vs N5)\")\n",
        "\n",
        "                escalamiento_por_tipo.append({\n",
        "                    'Problema': problema,\n",
        "                    **escalamiento_tipo,\n",
        "                    'Total_Repeats': sum(escalamiento_tipo.values())\n",
        "                })\n",
        "\n",
        "        escalamiento_tipo_df = pd.DataFrame(escalamiento_por_tipo)\n",
        "        if not escalamiento_tipo_df.empty:\n",
        "            escalamiento_tipo_df = escalamiento_tipo_df.sort_values('Total_Repeats', ascending=False)\n",
        "\n",
        "    # ==========================================\n",
        "    # 5. AN√ÅLISIS POR MERCADO\n",
        "    # ==========================================\n",
        "\n",
        "    print(f\"\\nüèôÔ∏è AN√ÅLISIS POR MERCADO:\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    if 'market' in df.columns:\n",
        "        market_analysis = df.groupby('market').agg({\n",
        "            'first_calls': 'sum',\n",
        "            'total_repeats': 'sum',\n",
        "            'total_calls': 'sum',\n",
        "            'market': 'count'  # casos\n",
        "        }).rename(columns={'market': 'casos'})\n",
        "\n",
        "        market_analysis['fcr_pct'] = (1 - (market_analysis['total_repeats'] / market_analysis['total_calls'])) * 100\n",
        "        market_analysis['repeat_rate'] = (market_analysis['total_repeats'] / market_analysis['total_calls']) * 100\n",
        "\n",
        "        print(\"Comparaci√≥n entre mercados:\")\n",
        "        for mercado, row in market_analysis.iterrows():\n",
        "            print(f\"{mercado}:\")\n",
        "            print(f\"  ‚Ä¢ FCR: {row['fcr_pct']:.1f}%\")\n",
        "            print(f\"  ‚Ä¢ Tasa repetidas: {row['repeat_rate']:.1f}%\")\n",
        "            print(f\"  ‚Ä¢ Total llamadas: {row['total_calls']:,}\")\n",
        "\n",
        "        # Identificar mercado con mejores/peores pr√°cticas\n",
        "        mejor_mercado = market_analysis.loc[market_analysis['fcr_pct'].idxmax()]\n",
        "        peor_mercado = market_analysis.loc[market_analysis['fcr_pct'].idxmin()]\n",
        "\n",
        "        print(f\"\\nüèÜ Mejor FCR: {mejor_mercado.name} ({mejor_mercado['fcr_pct']:.1f}%)\")\n",
        "        print(f\"‚ö†Ô∏è  Peor FCR: {peor_mercado.name} ({peor_mercado['fcr_pct']:.1f}%)\")\n",
        "\n",
        "        # An√°lisis cruzado: Mercado vs Tipo de Problema\n",
        "        print(f\"\\nüîç AN√ÅLISIS CRUZADO MERCADO vs PROBLEMA:\")\n",
        "        if 'problem_type_label' in df.columns:\n",
        "            cross_analysis = df.pivot_table(\n",
        "                values='total_repeats',\n",
        "                index='market',\n",
        "                columns='problem_type_label',\n",
        "                aggfunc='sum',\n",
        "                fill_value=0\n",
        "            )\n",
        "\n",
        "            print(\"Llamadas repetidas por mercado y problema:\")\n",
        "            print(cross_analysis)\n",
        "\n",
        "            # Identificar combinaciones problem√°ticas\n",
        "            max_val = cross_analysis.max().max()\n",
        "            max_locations = np.where(cross_analysis == max_val)\n",
        "            if len(max_locations[0]) > 0:\n",
        "                mercado_prob = cross_analysis.index[max_locations[0][0]]\n",
        "                problema_prob = cross_analysis.columns[max_locations[1][0]]\n",
        "                print(f\"\\nüéØ Combinaci√≥n m√°s problem√°tica: {mercado_prob} + {problema_prob} ({max_val:,} llamadas repetidas)\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 6. VALIDACI√ìN CONTRA TABLEAU\n",
        "    # ==========================================\n",
        "\n",
        "    print(f\"\\n‚úÖ VALIDACI√ìN FINAL:\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"¬øCoincide con Tableau?\")\n",
        "    print(f\"Total llamadas: {total_all_calls:,} (esperado: ~63,156)\")\n",
        "    print(f\"FCR: {fcr_rate:.1f}% (esperado: ~78.7%)\")\n",
        "    print(f\"Llamadas repetidas: {total_repeat_calls:,} (esperado: ~11,584)\")\n",
        "    print(f\"Ratio recontacto: {repeat_rate:.1f}% (esperado: ~21.3%)\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 7. RETORNAR DATOS CORREGIDOS\n",
        "    # ==========================================\n",
        "\n",
        "    return {\n",
        "        'df_corregido': df,\n",
        "        'metricas_principales': {\n",
        "            'total_calls': total_all_calls,\n",
        "            'fcr_rate': fcr_rate,\n",
        "            'repeat_rate': repeat_rate,\n",
        "            'total_repeats': total_repeat_calls\n",
        "        },\n",
        "        'escalamiento': escalamiento_df,\n",
        "        'por_problema': problem_analysis if 'problem_type' in df.columns else None,\n",
        "        'por_mercado': market_analysis if 'market' in df.columns else None\n",
        "    }\n",
        "\n",
        "def generate_escalamiento_viz(escalamiento_df, output_path='escalamiento_plot.png'):\n",
        "    \"\"\"\n",
        "    Crear visualizaci√≥n del patr√≥n de escalamiento N1‚ÜíN7\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Gr√°fico de l√≠nea para mostrar el patr√≥n\n",
        "    plt.plot(escalamiento_df['Contacto'], escalamiento_df['Llamadas'],\n",
        "             marker='o', linewidth=2, markersize=8)\n",
        "\n",
        "    plt.title('Patr√≥n de Escalamiento: Llamadas Repetidas por Contacto', fontsize=14)\n",
        "    plt.xlabel('N√∫mero de Contacto', fontsize=12)\n",
        "    plt.ylabel('N√∫mero de Llamadas', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Anotar valores\n",
        "    for i, row in escalamiento_df.iterrows():\n",
        "        plt.annotate(f\"{row['Llamadas']:,}\",\n",
        "                    (row['Contacto'], row['Llamadas']),\n",
        "                    textcoords=\"offset points\",\n",
        "                    xytext=(0,10),\n",
        "                    ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"üìä Gr√°fico guardado en: {output_path}\")\n",
        "\n",
        "# FUNCI√ìN PRINCIPAL PARA EJECUTAR TODO\n",
        "def run_complete_analysis(df):\n",
        "    \"\"\"\n",
        "    Ejecutar an√°lisis completo corregido\n",
        "    \"\"\"\n",
        "    print(\"üöÄ INICIANDO AN√ÅLISIS COMPLETO CORREGIDO...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    results = complete_corrected_analysis(df)\n",
        "\n",
        "    # Generar visualizaci√≥n del escalamiento\n",
        "    if results['escalamiento'] is not None and not results['escalamiento'].empty:\n",
        "        generate_escalamiento_viz(results['escalamiento'])\n",
        "\n",
        "    return results\n",
        "\n",
        "# ==========================================\n",
        "# EJECUCI√ìN AUTOM√ÅTICA COMPLETA\n",
        "# ==========================================\n",
        "\n",
        "def clean_and_analyze_google_fiber():\n",
        "    \"\"\"\n",
        "    Funci√≥n principal que ejecuta TODO el proceso autom√°ticamente\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Cargar y limpiar datos\n",
        "    print(\"üìä PASO 1: CARGANDO DATOS...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    url_market1 = 'https://docs.google.com/spreadsheets/d/1T7irtn0ay9MfuhG_6y2jEbeT-zLuEQTbv1qNxkpBaTE/export?format=csv'\n",
        "    url_market2 = 'https://docs.google.com/spreadsheets/d/1RE_MAKPt0JfWijbYCR32XFra9OGlmS2tLMxNs-X2NSU/export?format=csv'\n",
        "\n",
        "    try:\n",
        "        df1 = pd.read_csv(url_market1)\n",
        "        df2 = pd.read_csv(url_market2)\n",
        "        print(f\"‚úÖ Market 1: {len(df1)} registros\")\n",
        "        print(f\"‚úÖ Market 2: {len(df2)} registros\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error cargando datos: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 2. Estandarizar columnas\n",
        "    print(\"\\nüîß PASO 2: ESTANDARIZANDO DATOS...\")\n",
        "    df1.columns = df1.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "    df2.columns = df2.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "    # 3. Unir datasets\n",
        "    df = pd.concat([df1, df2], ignore_index=True)\n",
        "    print(f\"Dataset combinado: {len(df)} registros\")\n",
        "\n",
        "    # 4. Limpiar fechas\n",
        "    print(\"\\nüìÖ PASO 3: PROCESANDO FECHAS...\")\n",
        "    if 'date_created' in df.columns:\n",
        "        df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "        invalid_dates = df['date_created'].isnull().sum()\n",
        "        if invalid_dates > 0:\n",
        "            print(f\"‚ö†Ô∏è Eliminando {invalid_dates} registros con fechas inv√°lidas\")\n",
        "            df = df.dropna(subset=['date_created'])\n",
        "\n",
        "    # 5. Renombrar columnas principales\n",
        "    rename_mapping = {\n",
        "        'new_type': 'problem_type',\n",
        "        'new_market': 'market'\n",
        "    }\n",
        "    df = df.rename(columns=rename_mapping)\n",
        "\n",
        "    # 6. EJECUTAR AN√ÅLISIS COMPLETO\n",
        "    print(\"\\nüöÄ PASO 4: EJECUTANDO AN√ÅLISIS COMPLETO...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    results = complete_corrected_analysis(df)\n",
        "\n",
        "    # 7. Mostrar resumen final\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ RESUMEN EJECUTIVO FINAL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    metrics = results['metricas_principales']\n",
        "    print(f\"üìä M√âTRICAS VALIDADAS CONTRA TABLEAU:\")\n",
        "    print(f\"   ‚îú‚îÄ Total Llamadas: {metrics['total_calls']:,}\")\n",
        "    print(f\"   ‚îú‚îÄ FCR Rate: {metrics['fcr_rate']:.1f}%\")\n",
        "    print(f\"   ‚îú‚îÄ Llamadas Repetidas: {metrics['total_repeats']:,}\")\n",
        "    print(f\"   ‚îî‚îÄ Ratio Recontacto: {metrics['repeat_rate']:.1f}%\")\n",
        "\n",
        "    if results['por_problema'] is not None:\n",
        "        print(f\"\\nü•á PROBLEMA M√ÅS CR√çTICO:\")\n",
        "        worst_problem = results['por_problema'].iloc[0]\n",
        "        print(f\"   ‚îî‚îÄ {worst_problem.name}: {worst_problem['repeat_rate']:.1f}% tasa repetidas\")\n",
        "\n",
        "    if results['escalamiento'] is not None:\n",
        "        print(f\"\\nüìà PATR√ìN DE ESCALAMIENTO:\")\n",
        "        for _, row in results['escalamiento'].iterrows():\n",
        "            print(f\"   ‚îú‚îÄ {row['Contacto']}: {row['Llamadas']:,} llamadas ({row['Porcentaje']:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE!\")\n",
        "\n",
        "    return df, results\n",
        "\n",
        "# ==========================================\n",
        "# EJECUCI√ìN INMEDIATA AL CARGAR SCRIPT\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üé¨ EJECUTANDO AN√ÅLISIS COMPLETO DE GOOGLE FIBER...\")\n",
        "    print(\"üîÑ Procesando datos autom√°ticamente...\\n\")\n",
        "\n",
        "    # EJECUTAR TODO AUTOM√ÅTICAMENTE\n",
        "    df_final, analysis_results = clean_and_analyze_google_fiber()\n",
        "\n",
        "    if df_final is not None:\n",
        "        print(\"\\nüíæ GUARDANDO RESULTADOS...\")\n",
        "        df_final.to_csv('google_fiber_analysis_complete.csv', index=False)\n",
        "        print(\"üìÅ Archivo guardado: google_fiber_analysis_complete.csv\")\n",
        "\n",
        "        print(\"\\nüéâ ¬°PROCESO COMPLETADO!\")\n",
        "        print(\"üîç Revisa los resultados impresos arriba para validar contra Tableau\")\n",
        "    else:\n",
        "        print(\"‚ùå Error en el procesamiento\")\n",
        "\n",
        "# TAMBI√âN PUEDES EJECUTAR MANUALMENTE AS√ç:\n",
        "# df, results = clean_and_analyze_google_fiber()\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ EJECUTANDO AUTOM√ÅTICAMENTE...\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtsXgj0K7E-v",
        "outputId": "93f8e401-c443-4898-fea1-ec4726435494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé¨ EJECUTANDO AN√ÅLISIS COMPLETO DE GOOGLE FIBER...\n",
            "üîÑ Procesando datos autom√°ticamente...\n",
            "\n",
            "üìä PASO 1: CARGANDO DATOS...\n",
            "==================================================\n",
            "‚úÖ Market 1: 450 registros\n",
            "‚úÖ Market 2: 450 registros\n",
            "\n",
            "üîß PASO 2: ESTANDARIZANDO DATOS...\n",
            "Dataset combinado: 900 registros\n",
            "\n",
            "üìÖ PASO 3: PROCESANDO FECHAS...\n",
            "\n",
            "üöÄ PASO 4: EJECUTANDO AN√ÅLISIS COMPLETO...\n",
            "==================================================\n",
            "üîß CORRIGIENDO TODAS LAS M√âTRICAS...\n",
            "==================================================\n",
            "üìû Columna primera llamada: contacts_n\n",
            "üîÅ Columnas repetidas: ['contacts_n_1', 'contacts_n_2', 'contacts_n_3', 'contacts_n_4', 'contacts_n_5', 'contacts_n_6', 'contacts_n_7']\n",
            "\n",
            "üìä M√âTRICAS PRINCIPALES (CORREGIDAS):\n",
            "----------------------------------------\n",
            "Total de llamadas: 63,156.0\n",
            "‚îú‚îÄ Primera llamada: 49,722.0\n",
            "‚îî‚îÄ Llamadas repetidas: 13,434.0\n",
            "FCR Rate: 78.7%\n",
            "Repeat Rate: 21.3%\n",
            "Casos totales: 900\n",
            "‚îú‚îÄ Sin repetici√≥n: 252 (28.0%)\n",
            "‚îî‚îÄ Con repetici√≥n: 648 (72.0%)\n",
            "\n",
            "üîç AN√ÅLISIS DE ESCALAMIENTO POR CONTACTO:\n",
            "---------------------------------------------\n",
            "Contacto N1: 3,651.0 llamadas (27.2%)\n",
            "Contacto N2: 2,295.0 llamadas (17.1%)\n",
            "Contacto N3: 1,780.0 llamadas (13.2%)\n",
            "Contacto N4: 1,558.0 llamadas (11.6%)\n",
            "Contacto N5: 1,492.0 llamadas (11.1%)\n",
            "Contacto N6: 1,315.0 llamadas (9.8%)\n",
            "Contacto N7: 1,343.0 llamadas (10.0%)\n",
            "\n",
            "üéØ Pico m√°ximo en: N1 con 3,651.0 llamadas\n",
            "‚ö†Ô∏è  Aumentos detectados en: N7 (+2.1% vs N6)\n",
            "\n",
            "üìã AN√ÅLISIS POR TIPO DE PROBLEMA:\n",
            "-----------------------------------\n",
            "\n",
            "Ranking por Mayor Tasa de Repetici√≥n:\n",
            "Scheduling:\n",
            "  ‚Ä¢ Llamadas repetidas: 715.0 (33.3%)\n",
            "  ‚Ä¢ FCR: 66.7%\n",
            "  ‚Ä¢ Promedio llamadas/caso: 11.9\n",
            "Construction:\n",
            "  ‚Ä¢ Llamadas repetidas: 103.0 (25.3%)\n",
            "  ‚Ä¢ FCR: 74.7%\n",
            "  ‚Ä¢ Promedio llamadas/caso: 2.3\n",
            "Internet/WiFi:\n",
            "  ‚Ä¢ Llamadas repetidas: 6,365.0 (24.6%)\n",
            "  ‚Ä¢ FCR: 75.4%\n",
            "  ‚Ä¢ Promedio llamadas/caso: 143.9\n",
            "Account Manager:\n",
            "  ‚Ä¢ Llamadas repetidas: 902.0 (24.4%)\n",
            "  ‚Ä¢ FCR: 75.6%\n",
            "  ‚Ä¢ Promedio llamadas/caso: 20.5\n",
            "Technical:\n",
            "  ‚Ä¢ Llamadas repetidas: 5,349.0 (17.2%)\n",
            "  ‚Ä¢ FCR: 82.8%\n",
            "  ‚Ä¢ Promedio llamadas/caso: 172.3\n",
            "\n",
            "üîç ESCALAMIENTO POR TIPO DE PROBLEMA:\n",
            "\n",
            "üèôÔ∏è AN√ÅLISIS POR MERCADO:\n",
            "-------------------------\n",
            "Comparaci√≥n entre mercados:\n",
            "market_1:\n",
            "  ‚Ä¢ FCR: 78.2%\n",
            "  ‚Ä¢ Tasa repetidas: 21.8%\n",
            "  ‚Ä¢ Total llamadas: 57,980.0\n",
            "market_2:\n",
            "  ‚Ä¢ FCR: 84.8%\n",
            "  ‚Ä¢ Tasa repetidas: 15.2%\n",
            "  ‚Ä¢ Total llamadas: 5,176.0\n",
            "\n",
            "üèÜ Mejor FCR: market_2 (84.8%)\n",
            "‚ö†Ô∏è  Peor FCR: market_1 (78.2%)\n",
            "\n",
            "üîç AN√ÅLISIS CRUZADO MERCADO vs PROBLEMA:\n",
            "Llamadas repetidas por mercado y problema:\n",
            "problem_type_label  Account Manager  Construction  Internet/WiFi  Scheduling  \\\n",
            "market                                                                         \n",
            "market_1                      850.0          95.0         5969.0       691.0   \n",
            "market_2                       52.0           8.0          396.0        24.0   \n",
            "\n",
            "problem_type_label  Technical  \n",
            "market                         \n",
            "market_1               5042.0  \n",
            "market_2                307.0  \n",
            "\n",
            "üéØ Combinaci√≥n m√°s problem√°tica: market_1 + Internet/WiFi (5,969.0 llamadas repetidas)\n",
            "\n",
            "‚úÖ VALIDACI√ìN FINAL:\n",
            "--------------------\n",
            "¬øCoincide con Tableau?\n",
            "Total llamadas: 63,156.0 (esperado: ~63,156)\n",
            "FCR: 78.7% (esperado: ~78.7%)\n",
            "Llamadas repetidas: 13,434.0 (esperado: ~11,584)\n",
            "Ratio recontacto: 21.3% (esperado: ~21.3%)\n",
            "\n",
            "============================================================\n",
            "üéØ RESUMEN EJECUTIVO FINAL\n",
            "============================================================\n",
            "üìä M√âTRICAS VALIDADAS CONTRA TABLEAU:\n",
            "   ‚îú‚îÄ Total Llamadas: 63,156.0\n",
            "   ‚îú‚îÄ FCR Rate: 78.7%\n",
            "   ‚îú‚îÄ Llamadas Repetidas: 13,434.0\n",
            "   ‚îî‚îÄ Ratio Recontacto: 21.3%\n",
            "\n",
            "ü•á PROBLEMA M√ÅS CR√çTICO:\n",
            "   ‚îî‚îÄ Scheduling: 33.3% tasa repetidas\n",
            "\n",
            "üìà PATR√ìN DE ESCALAMIENTO:\n",
            "   ‚îú‚îÄ N1: 3,651.0 llamadas (27.2%)\n",
            "   ‚îú‚îÄ N2: 2,295.0 llamadas (17.1%)\n",
            "   ‚îú‚îÄ N3: 1,780.0 llamadas (13.2%)\n",
            "   ‚îú‚îÄ N4: 1,558.0 llamadas (11.6%)\n",
            "   ‚îú‚îÄ N5: 1,492.0 llamadas (11.1%)\n",
            "   ‚îú‚îÄ N6: 1,315.0 llamadas (9.8%)\n",
            "   ‚îú‚îÄ N7: 1,343.0 llamadas (10.0%)\n",
            "\n",
            "‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE!\n",
            "\n",
            "üíæ GUARDANDO RESULTADOS...\n",
            "üìÅ Archivo guardado: google_fiber_analysis_complete.csv\n",
            "\n",
            "üéâ ¬°PROCESO COMPLETADO!\n",
            "üîç Revisa los resultados impresos arriba para validar contra Tableau\n",
            "\n",
            "============================================================\n",
            "üöÄ EJECUTANDO AUTOM√ÅTICAMENTE...\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df.to_csv('/content/drive/MyDrive/google_fiber_clean.csv', index=False)\n"
      ],
      "metadata": {
        "id": "PR-yqTU7RL08",
        "outputId": "b80aad48-9f3b-4d60-abe5-2b7e7a89c1fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. OPCION SIMPLE DE LIMPIEZA DE DATOS"
      ],
      "metadata": {
        "id": "_obsfZ9MM9RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Cargar datos desde Google Sheets como CSV\n",
        "url_market1 = 'https://docs.google.com/spreadsheets/d/1T7irtn0ay9MfuhG_6y2jEbeT-zLuEQTbv1qNxkpBaTE/export?format=csv'\n",
        "url_market2 = 'https://docs.google.com/spreadsheets/d/1RE_MAKPt0JfWijbYCR32XFra9OGlmS2tLMxNs-X2NSU/export?format=csv'\n",
        "\n",
        "df1 = pd.read_csv(url_market1)\n",
        "df2 = pd.read_csv(url_market2)\n",
        "\n",
        "# 2. Estandarizar nombres de columnas\n",
        "df1.columns = df1.columns.str.strip().str.lower()\n",
        "df2.columns = df2.columns.str.strip().str.lower()\n",
        "\n",
        "# 3. Verificar estructura com√∫n\n",
        "assert set(df1.columns) == set(df2.columns), \"‚ö†Ô∏è Columnas no coinciden entre datasets\"\n",
        "\n",
        "# 4. Unir los dataframes\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# 5. Eliminar duplicados\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# 6. Verificar y convertir fecha\n",
        "df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "\n",
        "# 7. Eliminar filas con fechas nulas o contactos vac√≠os\n",
        "df = df.dropna(subset=['date_created', 'contacts_n'])\n",
        "\n",
        "# 8. Convertir columnas num√©ricas\n",
        "contact_cols = ['contacts_n', 'contacts_n_1', 'contacts_n_2', 'contacts_n_3',\n",
        "                'contacts_n_4', 'contacts_n_5', 'contacts_n_6', 'contacts_n_7']\n",
        "\n",
        "df[contact_cols] = df[contact_cols].fillna(0).astype(int)\n",
        "\n",
        "# 9. Renombrar columnas para claridad\n",
        "df = df.rename(columns={\n",
        "    'new_type': 'problem_type',\n",
        "    'new_market': 'market'\n",
        "})\n",
        "\n",
        "# 10. Calcular columnas clave\n",
        "repeat_cols = ['contacts_n_1', 'contacts_n_2', 'contacts_n_3',\n",
        "               'contacts_n_4', 'contacts_n_5', 'contacts_n_6', 'contacts_n_7']\n",
        "\n",
        "df['total_repeats'] = df[repeat_cols].sum(axis=1)\n",
        "df['total_calls'] = df['contacts_n'] + df['total_repeats']\n",
        "df['resolved_first_call'] = (df['total_repeats'] == 0).astype(int)  # 1 = resuelto en primera llamada\n",
        "\n",
        "# 11. Crear columna de semana y mes para an√°lisis temporal\n",
        "df['week'] = df['date_created'].dt.to_period('W').astype(str)\n",
        "df['month'] = df['date_created'].dt.to_period('M').astype(str)\n",
        "\n",
        "# 12. Revisar tipos y valores √∫nicos\n",
        "print(\"‚úÖ Datos limpios y listos. Vista previa:\")\n",
        "print(df.dtypes)\n",
        "print(df.head())\n",
        "\n",
        "# 13. Exportar CSV limpio\n",
        "df.to_csv('google_fiber_llamadas_limpias.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYDFu55nzeEu",
        "outputId": "8dfcaa7e-b515-49e7-837b-8d2dd79c2f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datos limpios y listos. Vista previa:\n",
            "date_created           datetime64[ns]\n",
            "contacts_n                      int64\n",
            "contacts_n_1                    int64\n",
            "contacts_n_2                    int64\n",
            "contacts_n_3                    int64\n",
            "contacts_n_4                    int64\n",
            "contacts_n_5                    int64\n",
            "contacts_n_6                    int64\n",
            "contacts_n_7                    int64\n",
            "problem_type                   object\n",
            "market                         object\n",
            "total_repeats                   int64\n",
            "total_calls                     int64\n",
            "resolved_first_call             int64\n",
            "week                           object\n",
            "month                          object\n",
            "dtype: object\n",
            "  date_created  contacts_n  contacts_n_1  contacts_n_2  contacts_n_3  \\\n",
            "0   2022-02-04         199            21             6            11   \n",
            "1   2022-01-30          19             2             0             2   \n",
            "2   2022-02-14          29             0             2             2   \n",
            "3   2022-01-16         120             6             6             5   \n",
            "4   2022-02-03         182            27            13             0   \n",
            "\n",
            "   contacts_n_4  contacts_n_5  contacts_n_6  contacts_n_7 problem_type  \\\n",
            "0             7            14             5             6       type_5   \n",
            "1             1             0             0             0       type_1   \n",
            "2             0             1             0             1       type_1   \n",
            "3             4             7             4             0       type_2   \n",
            "4            14             4             3             2       type_5   \n",
            "\n",
            "     market  total_repeats  total_calls  resolved_first_call  \\\n",
            "0  market_1             70          269                    0   \n",
            "1  market_1              5           24                    0   \n",
            "2  market_1              6           35                    0   \n",
            "3  market_1             32          152                    0   \n",
            "4  market_1             63          245                    0   \n",
            "\n",
            "                    week    month  \n",
            "0  2022-01-31/2022-02-06  2022-02  \n",
            "1  2022-01-24/2022-01-30  2022-01  \n",
            "2  2022-02-14/2022-02-20  2022-02  \n",
            "3  2022-01-10/2022-01-16  2022-01  \n",
            "4  2022-01-31/2022-02-06  2022-02  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similitudes y diferencias en los 2 analisis"
      ],
      "metadata": {
        "id": "YQ2Ry7OnOZ-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Similitudes\n",
        "\n",
        "Ambos:\n",
        "\n",
        "Cargan los dos datasets desde Google Sheets.\n",
        "\n",
        "Estandarizan nombres de columnas.\n",
        "\n",
        "Unifican los datasets en un solo DataFrame.\n",
        "\n",
        "Limpian duplicados y nulos b√°sicos.\n",
        "\n",
        "Procesan fechas (date_created).\n",
        "\n",
        "Normalizan columnas de contactos a enteros (contacts_n, contacts_n_1...).\n",
        "\n",
        "Calculan m√©tricas claves:\n",
        "\n",
        "total_repeats\n",
        "\n",
        "total_calls\n",
        "\n",
        "resolved_first_call\n",
        "\n",
        "Crean variables temporales (week, month en la simple; year, month, week, day_of_week en la avanzada).\n",
        "\n",
        "Exportan el resultado como CSV limpio.\n",
        "\n",
        "üîπ Diferencias clave\n",
        "Aspecto\tVersi√≥n simple\tVersi√≥n mejorada\n",
        "Carga de datos\tUsa pd.read_csv directamente, sin validaci√≥n.\tManeja errores con try/except y muestra el conteo de registros por mercado.\n",
        "Columnas\tassert para que ambas tablas tengan las mismas columnas. Si no coinciden, da error.\tM√°s flexible: muestra diferencias y trabaja solo con columnas comunes.\n",
        "Fechas\tConvierte date_created con un √∫nico intento (pd.to_datetime).\tIntenta m√∫ltiples formatos (%Y-%m-%d, %d/%m/%Y, etc.), reporta conversiones y elimina inv√°lidas.\n",
        "Duplicados\tSolo elimina filas duplicadas completas.\tDetecta duplicados por fecha, mercado y tipo de problema, y los agrega sumando contactos. Mucho m√°s realista.\n",
        "Contactos\tAsume columnas fijas (contacts_n, contacts_n_1...contacts_n_7).\tBusca din√°micamente todas las columnas que contengan \"contacts_\", escalable si el dataset cambia.\n",
        "Categ√≥ricas\tSolo renombra new_type ‚Üí problem_type y new_market ‚Üí market.\tLimpia espacios, reemplaza nan, null, None y reporta los valores √∫nicos para revisi√≥n.\n",
        "M√©tricas\tCalcula resolved_first_call (binario).\tAdem√°s:\n",
        "\n",
        "repeat_call_flag (llamadas repetidas)\n",
        "\n",
        "FCR rate (%)\n",
        "\n",
        "Repeat Call Rate (%) |\n",
        "| Variables temporales | week y month. | M√°s completas: year, month, week, day_of_week, date_only, year_month, year_week. |\n",
        "| Validaci√≥n | Solo imprime dtypes y head(). | Hace an√°lisis completo: rango de fechas, nulos restantes, estad√≠sticas descriptivas. |\n",
        "| Exportaci√≥n | Exporta un CSV. | Exporta CSV + genera un reporte de calidad en TXT con distribuci√≥n por mercado, tipo de problema, m√©tricas de rendimiento y columnas disponibles. |\n",
        "| Escalabilidad | Adecuada para datasets simples y controlados. | Robusta y adaptable a datasets heterog√©neos y en producci√≥n. |"
      ],
      "metadata": {
        "id": "FdlLL6p9OX02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":: # *json promp para otras limpiezas*:"
      ],
      "metadata": {
        "id": "3JNMuzzwPKuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"data_cleaning_steps\": [\n",
        "    {\n",
        "      \"step\": 1,\n",
        "      \"action\": \"load_data\",\n",
        "      \"details\": {\n",
        "        \"source\": [\"csv\", \"excel\", \"sql\", \"google_sheets\"],\n",
        "        \"error_handling\": \"try/except\",\n",
        "        \"log\": [\"rows\", \"columns\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 2,\n",
        "      \"action\": \"standardize_columns\",\n",
        "      \"details\": {\n",
        "        \"lowercase\": true,\n",
        "        \"strip_spaces\": true,\n",
        "        \"replace_spaces_with\": \"_\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 3,\n",
        "      \"action\": \"check_structure\",\n",
        "      \"details\": {\n",
        "        \"validate_columns\": true,\n",
        "        \"handle_differences\": [\"report\", \"keep_common\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 4,\n",
        "      \"action\": \"merge_datasets\",\n",
        "      \"details\": {\n",
        "        \"method\": \"concat\",\n",
        "        \"ignore_index\": true\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 5,\n",
        "      \"action\": \"analyze_nulls\",\n",
        "      \"details\": {\n",
        "        \"metrics\": [\"count\", \"percentage\"],\n",
        "        \"strategy\": [\"drop\", \"impute\", \"keep\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 6,\n",
        "      \"action\": \"process_dates\",\n",
        "      \"details\": {\n",
        "        \"formats\": [\"%Y-%m-%d\", \"%d/%m/%Y\", \"%m/%d/%Y\"],\n",
        "        \"convert_to\": \"datetime\",\n",
        "        \"new_features\": [\"year\", \"month\", \"week\", \"day_of_week\", \"periods\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 7,\n",
        "      \"action\": \"process_numeric\",\n",
        "      \"details\": {\n",
        "        \"convert\": \"pd.to_numeric\",\n",
        "        \"fillna\": 0,\n",
        "        \"types\": [\"int\", \"float\"],\n",
        "        \"outliers\": \"optional\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 8,\n",
        "      \"action\": \"process_categorical\",\n",
        "      \"details\": {\n",
        "        \"clean_spaces\": true,\n",
        "        \"lowercase\": true,\n",
        "        \"replace_invalid\": [\"NaN\", \"null\", \"None\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 9,\n",
        "      \"action\": \"handle_duplicates\",\n",
        "      \"details\": {\n",
        "        \"subset\": [\"id\", \"date\", \"category\"],\n",
        "        \"strategy\": [\"drop\", \"aggregate\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 10,\n",
        "      \"action\": \"generate_metrics\",\n",
        "      \"details\": {\n",
        "        \"totals\": [\"sum_columns\"],\n",
        "        \"flags\": [\"binary_indicators\"],\n",
        "        \"ratios\": [\"conversion_rate\", \"repeat_rate\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 11,\n",
        "      \"action\": \"final_validation\",\n",
        "      \"details\": {\n",
        "        \"check_nulls\": true,\n",
        "        \"check_date_range\": true,\n",
        "        \"summary\": [\"df.info\", \"df.describe\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 12,\n",
        "      \"action\": \"export_data\",\n",
        "      \"details\": {\n",
        "        \"format\": [\"csv\", \"excel\", \"db\"],\n",
        "        \"filename\": \"dataset_clean_DATE.csv\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 13,\n",
        "      \"action\": \"generate_report\",\n",
        "      \"details\": {\n",
        "        \"include\": [\n",
        "          \"initial_vs_final_records\",\n",
        "          \"null_handling_summary\",\n",
        "          \"categorical_distribution\",\n",
        "          \"key_metrics\"\n",
        "        ],\n",
        "        \"output\": \"txt\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "4qGKtfrWPJuz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}