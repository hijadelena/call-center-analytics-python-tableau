{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqi/Gr9aGqsNVwzAFfJIMs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hijadelena/call-center-analytics-python-tableau/blob/main/EDA_fiber.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🧩Dashboard de Análisis de Llamadas Repetidas al Servicio de Atención al Cliente**"
      ],
      "metadata": {
        "id": "5gPCxM_xnrMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Limpieza y carga de datos, los pasos que hacemos**\n",
        "\n",
        "Cargar los datasets desde Google Sheets\n",
        "\n",
        "Estandarizar nombres de columnas\n",
        "\n",
        "Eliminar duplicados y valores nulos\n",
        "\n",
        "Convertir columnas de fecha\n",
        "\n",
        "Verificar tipos de datos\n",
        "\n",
        "Añadir columnas calculadas útiles para Tableau\n",
        "\n"
      ],
      "metadata": {
        "id": "qUhAwOGRn1MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "def clean_google_fiber_data():\n",
        "    \"\"\"\n",
        "    Limpieza mejorada de datos de Google Fiber con manejo robusto de nulos\n",
        "    y agregación de fechas duplicadas\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Cargar datos desde Google Sheets\n",
        "    print(\"📊 Cargando datos de Google Sheets...\")\n",
        "    url_market1 = 'https://docs.google.com/spreadsheets/d/1T7irtn0ay9MfuhG_6y2jEbeT-zLuEQTbv1qNxkpBaTE/export?format=csv'\n",
        "    url_market2 = 'https://docs.google.com/spreadsheets/d/1RE_MAKPt0JfWijbYCR32XFra9OGlmS2tLMxNs-X2NSU/export?format=csv'\n",
        "\n",
        "    try:\n",
        "        df1 = pd.read_csv(url_market1)\n",
        "        df2 = pd.read_csv(url_market2)\n",
        "        print(f\"✅ Market 1: {len(df1)} registros\")\n",
        "        print(f\"✅ Market 2: {len(df2)} registros\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error cargando datos: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 2. Estandarizar nombres de columnas\n",
        "    print(\"\\n🔧 Estandarizando columnas...\")\n",
        "    df1.columns = df1.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "    df2.columns = df2.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "    # 3. Verificar estructura común\n",
        "    if set(df1.columns) != set(df2.columns):\n",
        "        print(\"⚠️ Diferencias en columnas:\")\n",
        "        print(f\"Solo en Market 1: {set(df1.columns) - set(df2.columns)}\")\n",
        "        print(f\"Solo en Market 2: {set(df2.columns) - set(df1.columns)}\")\n",
        "        # Tomar columnas comunes\n",
        "        common_cols = list(set(df1.columns) & set(df2.columns))\n",
        "        df1 = df1[common_cols]\n",
        "        df2 = df2[common_cols]\n",
        "\n",
        "    # 4. Unir dataframes\n",
        "    print(\"🔗 Uniendo datasets...\")\n",
        "    df = pd.concat([df1, df2], ignore_index=True)\n",
        "    print(f\"Dataset combinado: {len(df)} registros\")\n",
        "\n",
        "    # 5. Análisis inicial de nulos\n",
        "    print(\"\\n📋 Análisis inicial de valores nulos:\")\n",
        "    null_counts = df.isnull().sum()\n",
        "    null_percentages = (df.isnull().sum() / len(df)) * 100\n",
        "    null_summary = pd.DataFrame({\n",
        "        'Nulos': null_counts,\n",
        "        'Porcentaje': null_percentages\n",
        "    }).sort_values('Porcentaje', ascending=False)\n",
        "    print(null_summary[null_summary['Nulos'] > 0])\n",
        "\n",
        "    # 6. Limpiar y convertir fecha (initial attempt)\n",
        "    print(\"\\n📅 Procesando fechas...\")\n",
        "    if 'date_created' in df.columns:\n",
        "        # Probar múltiples formatos de fecha\n",
        "        date_formats = ['%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%Y-%m-%d %H:%M:%S']\n",
        "        df['date_created_clean'] = None\n",
        "\n",
        "        for fmt in date_formats:\n",
        "            mask = df['date_created_clean'].isnull()\n",
        "            try:\n",
        "                df.loc[mask, 'date_created_clean'] = pd.to_datetime(\n",
        "                    df.loc[mask, 'date_created'],\n",
        "                    format=fmt,\n",
        "                    errors='coerce'\n",
        "                )\n",
        "                converted = mask.sum() - df['date_created_clean'].isnull().sum()\n",
        "                if converted > 0:\n",
        "                    print(f\"✅ Convertidas {converted} fechas con formato {fmt}\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Eliminar registros con fechas inválidas\n",
        "        invalid_dates = df['date_created_clean'].isnull().sum()\n",
        "        if invalid_dates > 0:\n",
        "            print(f\"⚠️ Eliminando {invalid_dates} registros con fechas inválidas\")\n",
        "            df = df.dropna(subset=['date_created_clean'])\n",
        "\n",
        "        df['date_created'] = df['date_created_clean']\n",
        "        df = df.drop('date_created_clean', axis=1)\n",
        "        # Ensure date_created is datetime after initial cleaning\n",
        "        df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "\n",
        "\n",
        "    # 7. Identificar y limpiar columnas de contactos\n",
        "    print(\"\\n☎️ Procesando columnas de contactos...\")\n",
        "    contact_columns = [col for col in df.columns if 'contacts_' in col]\n",
        "    print(f\"Columnas de contactos encontradas: {contact_columns}\")\n",
        "\n",
        "    # Convertir a numérico y llenar nulos con 0\n",
        "    for col in contact_columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # 8. Limpiar columnas categóricas\n",
        "    print(\"\\n🏷️ Limpiando columnas categóricas...\")\n",
        "    categorical_columns = []\n",
        "\n",
        "    # Identificar columnas de tipo/mercado\n",
        "    for col in df.columns:\n",
        "        if any(keyword in col.lower() for keyword in ['type', 'market', 'new_']):\n",
        "            categorical_columns.append(col)\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        if col in df.columns:\n",
        "            # Limpiar espacios y valores vacíos\n",
        "            df[col] = df[col].astype(str).str.strip()\n",
        "            df[col] = df[col].replace(['', 'nan', 'None', 'null'], np.nan)\n",
        "\n",
        "            # Mostrar valores únicos para revisión\n",
        "            unique_values = df[col].value_counts(dropna=False)\n",
        "            print(f\"\\n{col} - Valores únicos:\")\n",
        "            print(unique_values.head(10))\n",
        "\n",
        "    # 9. Manejar fechas duplicadas - AGREGACIÓN\n",
        "    print(\"\\n📊 Manejando fechas duplicadas...\")\n",
        "\n",
        "    # Definir columnas de agrupación (excluyendo contactos)\n",
        "    group_columns = ['date_created']\n",
        "    if 'new_market' in df.columns:\n",
        "        group_columns.append('new_market')\n",
        "    if 'new_type' in df.columns:\n",
        "        group_columns.append('new_type')\n",
        "\n",
        "    # Ensure group_columns are in df.columns before grouping\n",
        "    group_columns = [col for col in group_columns if col in df.columns]\n",
        "\n",
        "    # Convert date_created to datetime before checking for duplicates\n",
        "    df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "\n",
        "\n",
        "    # Identify duplicates before aggregation\n",
        "    duplicates_before = df.duplicated(subset=group_columns).sum()\n",
        "    print(f\"Registros duplicados por fecha/mercado/tipo: {duplicates_before}\")\n",
        "\n",
        "    if duplicates_before > 0:\n",
        "        print(\"🔄 Agregando datos por fecha/mercado/tipo...\")\n",
        "\n",
        "        # Agregación: sumar columnas de contactos\n",
        "        agg_dict = {}\n",
        "        for col in contact_columns:\n",
        "            agg_dict[col] = 'sum'\n",
        "\n",
        "        # Maintain first occurrence for other columns, excluding date_created\n",
        "        for col in df.columns:\n",
        "            if col not in contact_columns + group_columns and col != 'date_created':\n",
        "                agg_dict[col] = 'first'\n",
        "\n",
        "        df_aggregated = df.groupby(group_columns, as_index=False).agg(agg_dict)\n",
        "        print(f\"✅ Datos agregados: {len(df)} → {len(df_aggregated)} registros\")\n",
        "        df = df_aggregated\n",
        "        # Ensure date_created is datetime after aggregation\n",
        "        df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "\n",
        "\n",
        "    # 10. Renombrar columnas para claridad\n",
        "    print(\"\\n🏷️ Renombrando columnas...\")\n",
        "    rename_mapping = {\n",
        "        'new_type': 'problem_type',\n",
        "        'new_market': 'market'\n",
        "    }\n",
        "    df = df.rename(columns=rename_mapping)\n",
        "\n",
        "    # 11. Calcular métricas clave\n",
        "    print(\"\\n📈 Calculando métricas...\")\n",
        "\n",
        "    # Identificar columnas de llamadas repetidas\n",
        "    repeat_columns = [col for col in contact_columns if '_n_' in col and col != 'contacts_n']\n",
        "\n",
        "    # Calcular totales\n",
        "    df['total_repeats'] = df[repeat_columns].sum(axis=1)\n",
        "    df['total_calls'] = df['contacts_n'] + df['total_repeats']\n",
        "    df['resolved_first_call'] = (df['total_repeats'] == 0).astype(int)\n",
        "    df['repeat_call_flag'] = (df['total_repeats'] > 0).astype(int)\n",
        "\n",
        "    # Calcular tasa de resolución en primera llamada\n",
        "    if len(df) > 0:\n",
        "        fcr_rate = (df['resolved_first_call'].sum() / len(df)) * 100\n",
        "        repeat_rate = (df['repeat_call_flag'].sum() / len(df)) * 100\n",
        "        print(f\"📊 FCR Rate: {fcr_rate:.2f}%\")\n",
        "        print(f\"📊 Repeat Call Rate: {repeat_rate:.2f}%\")\n",
        "\n",
        "    # 12. Crear variables temporales para análisis\n",
        "    print(\"\\n📅 Creando variables temporales...\")\n",
        "    # Ensure date_created is datetime before using .dt accessor\n",
        "    df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "    df['year'] = df['date_created'].dt.year\n",
        "    df['month'] = df['date_created'].dt.month\n",
        "    df['week'] = df['date_created'].dt.isocalendar().week\n",
        "    df['day_of_week'] = df['date_created'].dt.day_name()\n",
        "    df['date_only'] = df['date_created'].dt.date\n",
        "\n",
        "    # Períodos para agrupación\n",
        "    df['year_month'] = df['date_created'].dt.to_period('M').astype(str)\n",
        "    df['year_week'] = df['date_created'].dt.to_period('W').astype(str)\n",
        "\n",
        "    # 13. Validación final\n",
        "    print(\"\\n✅ Validación final...\")\n",
        "    print(f\"Registros finales: {len(df)}\")\n",
        "    print(f\"Rango de fechas: {df['date_created'].min()} - {df['date_created'].max()}\")\n",
        "    print(f\"Nulos restantes por columna:\")\n",
        "\n",
        "    final_nulls = df.isnull().sum()\n",
        "    if final_nulls.sum() > 0:\n",
        "        print(final_nulls[final_nulls > 0])\n",
        "    else:\n",
        "        print(\"Sin valores nulos ✅\")\n",
        "\n",
        "    # 14. Estadísticas descriptivas\n",
        "    print(\"\\n📊 Estadísticas descriptivas:\")\n",
        "    numeric_cols = ['contacts_n', 'total_repeats', 'total_calls']\n",
        "    print(df[numeric_cols].describe())\n",
        "\n",
        "    # 15. Exportar datos limpios\n",
        "    output_file = 'google_fiber_clean.csv'\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"\\n💾 Datos exportados a: {output_file}\")\n",
        "\n",
        "    # 16. Generar reporte de calidad\n",
        "    generate_data_quality_report(df, output_file.replace('.csv', '_quality_report.txt'))\n",
        "\n",
        "    return df\n",
        "\n",
        "def generate_data_quality_report(df, filename):\n",
        "    \"\"\"Generar reporte de calidad de datos\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"GOOGLE FIBER - REPORTE DE CALIDAD DE DATOS\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Fecha de generación: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "        f.write(\"1. RESUMEN GENERAL\\n\")\n",
        "        f.write(\"-\" * 20 + \"\\n\")\n",
        "        f.write(f\"Total de registros: {len(df):,}\\n\")\n",
        "        f.write(f\"Total de columnas: {len(df.columns)}\\n\")\n",
        "        f.write(f\"Rango de fechas: {df['date_created'].min()} - {df['date_created'].max()}\\n\")\n",
        "        f.write(f\"Período total: {(df['date_created'].max() - df['date_created'].min()).days} días\\n\\n\")\n",
        "\n",
        "        f.write(\"2. DISTRIBUCIÓN POR MERCADO\\n\")\n",
        "        f.write(\"-\" * 30 + \"\\n\")\n",
        "        if 'market' in df.columns:\n",
        "            market_dist = df['market'].value_counts()\n",
        "            for market, count in market_dist.items():\n",
        "                f.write(f\"{market}: {count:,} registros ({count/len(df)*100:.1f}%)\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"3. DISTRIBUCIÓN POR TIPO DE PROBLEMA\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        if 'problem_type' in df.columns:\n",
        "            type_dist = df['problem_type'].value_counts()\n",
        "            for ptype, count in type_dist.items():\n",
        "                f.write(f\"{ptype}: {count:,} registros ({count/len(df)*100:.1f}%)\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"4. MÉTRICAS DE RENDIMIENTO\\n\")\n",
        "        f.write(\"-\" * 30 + \"\\n\")\n",
        "        fcr_rate = (df['resolved_first_call'].sum() / len(df)) * 100\n",
        "        repeat_rate = (df['repeat_call_flag'].sum() / len(df)) * 100\n",
        "        avg_calls = df['total_calls'].mean()\n",
        "\n",
        "        f.write(f\"Tasa de resolución en primera llamada (FCR): {fcr_rate:.2f}%\\n\")\n",
        "        f.write(f\"Tasa de llamadas repetidas: {repeat_rate:.2f}%\\n\")\n",
        "        f.write(f\"Promedio de llamadas por caso: {avg_calls:.2f}\\n\\n\")\n",
        "\n",
        "        f.write(\"5. COLUMNAS DISPONIBLES PARA TABLEAU\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for col in sorted(df.columns):\n",
        "            dtype = str(df[col].dtype)\n",
        "            nulls = df[col].isnull().sum()\n",
        "            f.write(f\"{col} ({dtype}) - Nulos: {nulls}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Iniciando limpieza de datos Google Fiber...\")\n",
        "    clean_data = clean_google_fiber_data()\n",
        "    print(\"\\n🎉 Proceso completado!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piu_9Zm7NNZe",
        "outputId": "633f0bc4-8ffa-41a1-901e-537be19e8a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Iniciando limpieza de datos Google Fiber...\n",
            "📊 Cargando datos de Google Sheets...\n",
            "✅ Market 1: 450 registros\n",
            "✅ Market 2: 450 registros\n",
            "\n",
            "🔧 Estandarizando columnas...\n",
            "🔗 Uniendo datasets...\n",
            "Dataset combinado: 900 registros\n",
            "\n",
            "📋 Análisis inicial de valores nulos:\n",
            "              Nulos  Porcentaje\n",
            "contacts_n_6    445   49.444444\n",
            "contacts_n_5    439   48.777778\n",
            "contacts_n_7    435   48.333333\n",
            "contacts_n_4    432   48.000000\n",
            "contacts_n_3    404   44.888889\n",
            "contacts_n_2    374   41.555556\n",
            "contacts_n_1    328   36.444444\n",
            "contacts_n      132   14.666667\n",
            "\n",
            "📅 Procesando fechas...\n",
            "✅ Convertidas 900 fechas con formato %Y-%m-%d\n",
            "\n",
            "☎️ Procesando columnas de contactos...\n",
            "Columnas de contactos encontradas: ['contacts_n', 'contacts_n_1', 'contacts_n_2', 'contacts_n_3', 'contacts_n_4', 'contacts_n_5', 'contacts_n_6', 'contacts_n_7']\n",
            "\n",
            "🏷️ Limpiando columnas categóricas...\n",
            "\n",
            "new_type - Valores únicos:\n",
            "new_type\n",
            "type_5    180\n",
            "type_1    180\n",
            "type_2    180\n",
            "type_4    180\n",
            "type_3    180\n",
            "Name: count, dtype: int64\n",
            "\n",
            "new_market - Valores únicos:\n",
            "new_market\n",
            "market_1    450\n",
            "market_2    450\n",
            "Name: count, dtype: int64\n",
            "\n",
            "📊 Manejando fechas duplicadas...\n",
            "Registros duplicados por fecha/mercado/tipo: 0\n",
            "\n",
            "🏷️ Renombrando columnas...\n",
            "\n",
            "📈 Calculando métricas...\n",
            "📊 FCR Rate: 28.00%\n",
            "📊 Repeat Call Rate: 72.00%\n",
            "\n",
            "📅 Creando variables temporales...\n",
            "\n",
            "✅ Validación final...\n",
            "Registros finales: 900\n",
            "Rango de fechas: 2022-01-01 00:00:00 - 2022-03-31 00:00:00\n",
            "Nulos restantes por columna:\n",
            "Sin valores nulos ✅\n",
            "\n",
            "📊 Estadísticas descriptivas:\n",
            "       contacts_n  total_repeats  total_calls\n",
            "count  900.000000     900.000000   900.000000\n",
            "mean    55.246667      14.926667    70.173333\n",
            "std     95.214851      25.155869   119.002823\n",
            "min      0.000000       0.000000     0.000000\n",
            "25%      2.000000       0.000000     2.000000\n",
            "50%     15.000000       3.000000    19.000000\n",
            "75%     33.000000      11.000000    42.250000\n",
            "max    599.000000     119.000000   683.000000\n",
            "\n",
            "💾 Datos exportados a: google_fiber_clean.csv\n",
            "\n",
            "🎉 Proceso completado!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----CORRECCION DE METRICAS----\n",
        "se encontro que el fcr y otras metricas tenian, Error en el código Python:\n",
        "pythondf['resolved_first_call'] = (df['total_repeats'] == 0).astype(int)\n",
        "Esta lógica considera que si total_repeats == 0, entonces se resolvió en primera llamada. Pero esto es incorrecto porque:\n",
        "\n",
        "Un registro puede tener contacts_n = 0 (sin primera llamada) y total_repeats = 0\n",
        "Esto marcaría erróneamente como \"resuelto en primera llamada\"\\"
      ],
      "metadata": {
        "id": "CZLR1Dt-69ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "def complete_corrected_analysis(df):\n",
        "    \"\"\"\n",
        "    Análisis completo con TODAS las métricas corregidas + análisis de patrones\n",
        "    \"\"\"\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. CORRECCIÓN COMPLETA DE MÉTRICAS\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"🔧 CORRIGIENDO TODAS LAS MÉTRICAS...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Identificar columnas de contactos\n",
        "    contact_columns = [col for col in df.columns if 'contacts_n' in col]\n",
        "    repeat_columns = [col for col in contact_columns if 'contacts_n_' in col]\n",
        "\n",
        "    print(f\"📞 Columna primera llamada: contacts_n\")\n",
        "    print(f\"🔁 Columnas repetidas: {repeat_columns}\")\n",
        "\n",
        "    # Calcular totales CORRECTOS\n",
        "    df['first_calls'] = df['contacts_n'].fillna(0)\n",
        "    df['repeat_n1'] = df['contacts_n_1'].fillna(0) if 'contacts_n_1' in df.columns else 0\n",
        "    df['repeat_n2'] = df['contacts_n_2'].fillna(0) if 'contacts_n_2' in df.columns else 0\n",
        "    df['repeat_n3'] = df['contacts_n_3'].fillna(0) if 'contacts_n_3' in df.columns else 0\n",
        "    df['repeat_n4'] = df['contacts_n_4'].fillna(0) if 'contacts_n_4' in df.columns else 0\n",
        "    df['repeat_n5'] = df['contacts_n_5'].fillna(0) if 'contacts_n_5' in df.columns else 0\n",
        "    df['repeat_n6'] = df['contacts_n_6'].fillna(0) if 'contacts_n_6' in df.columns else 0\n",
        "    df['repeat_n7'] = df['contacts_n_7'].fillna(0) if 'contacts_n_7' in df.columns else 0\n",
        "\n",
        "    # Total repetidas y total llamadas\n",
        "    df['total_repeats'] = df[repeat_columns].sum(axis=1)\n",
        "    df['total_calls'] = df['first_calls'] + df['total_repeats']\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. MÉTRICAS CORREGIDAS (COMO TABLEAU)\n",
        "    # ==========================================\n",
        "\n",
        "    # Totales agregados\n",
        "    total_first_calls = df['first_calls'].sum()\n",
        "    total_repeat_calls = df['total_repeats'].sum()\n",
        "    total_all_calls = df['total_calls'].sum()\n",
        "\n",
        "    # FCR y Repeat Rate (como Tableau)\n",
        "    fcr_rate = (1 - (total_repeat_calls / total_all_calls)) * 100 if total_all_calls > 0 else 0\n",
        "    repeat_rate = (total_repeat_calls / total_all_calls) * 100 if total_all_calls > 0 else 0\n",
        "\n",
        "    # Casos únicos vs llamadas totales\n",
        "    total_cases = len(df)\n",
        "    cases_no_repeat = (df['total_repeats'] == 0).sum()\n",
        "    cases_with_repeat = (df['total_repeats'] > 0).sum()\n",
        "\n",
        "    print(\"\\n📊 MÉTRICAS PRINCIPALES (CORREGIDAS):\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Total de llamadas: {total_all_calls:,}\")\n",
        "    print(f\"├─ Primera llamada: {total_first_calls:,}\")\n",
        "    print(f\"└─ Llamadas repetidas: {total_repeat_calls:,}\")\n",
        "    print(f\"FCR Rate: {fcr_rate:.1f}%\")\n",
        "    print(f\"Repeat Rate: {repeat_rate:.1f}%\")\n",
        "    print(f\"Casos totales: {total_cases:,}\")\n",
        "    print(f\"├─ Sin repetición: {cases_no_repeat:,} ({cases_no_repeat/total_cases*100:.1f}%)\")\n",
        "    print(f\"└─ Con repetición: {cases_with_repeat:,} ({cases_with_repeat/total_cases*100:.1f}%)\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. ANÁLISIS DE ESCALAMIENTO (N1 → N7)\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"\\n🔍 ANÁLISIS DE ESCALAMIENTO POR CONTACTO:\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    escalamiento_data = []\n",
        "    for i, col in enumerate(repeat_columns, 1):\n",
        "        if col in df.columns:\n",
        "            total_contacto = df[col].sum()\n",
        "            pct_del_total = (total_contacto / total_repeat_calls * 100) if total_repeat_calls > 0 else 0\n",
        "            escalamiento_data.append({\n",
        "                'Contacto': f'N{i}',\n",
        "                'Llamadas': total_contacto,\n",
        "                'Porcentaje': pct_del_total\n",
        "            })\n",
        "            print(f\"Contacto N{i}: {total_contacto:,} llamadas ({pct_del_total:.1f}%)\")\n",
        "\n",
        "    escalamiento_df = pd.DataFrame(escalamiento_data)\n",
        "\n",
        "    # Detectar picos de escalamiento\n",
        "    if len(escalamiento_df) > 0:\n",
        "        max_contacto = escalamiento_df.loc[escalamiento_df['Llamadas'].idxmax()]\n",
        "        print(f\"\\n🎯 Pico máximo en: {max_contacto['Contacto']} con {max_contacto['Llamadas']:,} llamadas\")\n",
        "\n",
        "        # Identificar aumentos inusuales (como el N6 que mencionas)\n",
        "        aumentos = []\n",
        "        for i in range(1, len(escalamiento_df)):\n",
        "            anterior = escalamiento_df.iloc[i-1]['Llamadas']\n",
        "            actual = escalamiento_df.iloc[i]['Llamadas']\n",
        "            if actual > anterior:\n",
        "                aumento_pct = ((actual - anterior) / anterior * 100) if anterior > 0 else 0\n",
        "                aumentos.append(f\"N{i+1} (+{aumento_pct:.1f}% vs N{i})\")\n",
        "\n",
        "        if aumentos:\n",
        "            print(f\"⚠️  Aumentos detectados en: {', '.join(aumentos)}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. ANÁLISIS POR TIPO DE PROBLEMA\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"\\n📋 ANÁLISIS POR TIPO DE PROBLEMA:\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    # Mapeo de tipos (según tu información)\n",
        "    type_mapping = {\n",
        "        'type_1': 'Account Manager',\n",
        "        'type_2': 'Technical',\n",
        "        'type_3': 'Scheduling',\n",
        "        'type_4': 'Construction',\n",
        "        'type_5': 'Internet/WiFi'\n",
        "    }\n",
        "\n",
        "    if 'problem_type' in df.columns:\n",
        "        # Aplicar mapeo si es necesario\n",
        "        df['problem_type_label'] = df['problem_type'].map(type_mapping).fillna(df['problem_type'])\n",
        "\n",
        "        problem_analysis = df.groupby('problem_type_label').agg({\n",
        "            'first_calls': 'sum',\n",
        "            'total_repeats': 'sum',\n",
        "            'total_calls': 'sum',\n",
        "            'problem_type': 'count'  # casos\n",
        "        }).rename(columns={'problem_type': 'casos'})\n",
        "\n",
        "        problem_analysis['fcr_pct'] = (1 - (problem_analysis['total_repeats'] / problem_analysis['total_calls'])) * 100\n",
        "        problem_analysis['repeat_rate'] = (problem_analysis['total_repeats'] / problem_analysis['total_calls']) * 100\n",
        "        problem_analysis['avg_calls_per_case'] = problem_analysis['total_calls'] / problem_analysis['casos']\n",
        "\n",
        "        # Ordenar por problemas más problemáticos\n",
        "        problem_analysis = problem_analysis.sort_values('repeat_rate', ascending=False)\n",
        "\n",
        "        print(\"\\nRanking por Mayor Tasa de Repetición:\")\n",
        "        for idx, row in problem_analysis.iterrows():\n",
        "            print(f\"{idx}:\")\n",
        "            print(f\"  • Llamadas repetidas: {row['total_repeats']:,} ({row['repeat_rate']:.1f}%)\")\n",
        "            print(f\"  • FCR: {row['fcr_pct']:.1f}%\")\n",
        "            print(f\"  • Promedio llamadas/caso: {row['avg_calls_per_case']:.1f}\")\n",
        "\n",
        "        # Análisis de escalamiento por tipo\n",
        "        print(f\"\\n🔍 ESCALAMIENTO POR TIPO DE PROBLEMA:\")\n",
        "        escalamiento_por_tipo = []\n",
        "\n",
        "        for problema in df['problem_type_label'].unique():\n",
        "            if pd.notna(problema):\n",
        "                subset = df[df['problem_type_label'] == problema]\n",
        "                escalamiento_tipo = {}\n",
        "\n",
        "                for i, col in enumerate(repeat_columns, 1):\n",
        "                    if col in subset.columns:\n",
        "                        escalamiento_tipo[f'N{i}'] = subset[col].sum()\n",
        "\n",
        "                # Buscar picos en N6 específicamente\n",
        "                if 'N6' in escalamiento_tipo and len(escalamiento_tipo) > 5:\n",
        "                    n5_val = escalamiento_tipo.get('N5', 0)\n",
        "                    n6_val = escalamiento_tipo.get('N6', 0)\n",
        "                    n7_val = escalamiento_tipo.get('N7', 0)\n",
        "\n",
        "                    if n6_val > n5_val and n6_val > n7_val and n6_val > 0:\n",
        "                        pct_aumento = ((n6_val - n5_val) / n5_val * 100) if n5_val > 0 else 0\n",
        "                        print(f\"  ⚠️  {problema}: Pico en N6 ({n6_val:,} llamadas, +{pct_aumento:.1f}% vs N5)\")\n",
        "\n",
        "                escalamiento_por_tipo.append({\n",
        "                    'Problema': problema,\n",
        "                    **escalamiento_tipo,\n",
        "                    'Total_Repeats': sum(escalamiento_tipo.values())\n",
        "                })\n",
        "\n",
        "        escalamiento_tipo_df = pd.DataFrame(escalamiento_por_tipo)\n",
        "        if not escalamiento_tipo_df.empty:\n",
        "            escalamiento_tipo_df = escalamiento_tipo_df.sort_values('Total_Repeats', ascending=False)\n",
        "\n",
        "    # ==========================================\n",
        "    # 5. ANÁLISIS POR MERCADO\n",
        "    # ==========================================\n",
        "\n",
        "    print(f\"\\n🏙️ ANÁLISIS POR MERCADO:\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    if 'market' in df.columns:\n",
        "        market_analysis = df.groupby('market').agg({\n",
        "            'first_calls': 'sum',\n",
        "            'total_repeats': 'sum',\n",
        "            'total_calls': 'sum',\n",
        "            'market': 'count'  # casos\n",
        "        }).rename(columns={'market': 'casos'})\n",
        "\n",
        "        market_analysis['fcr_pct'] = (1 - (market_analysis['total_repeats'] / market_analysis['total_calls'])) * 100\n",
        "        market_analysis['repeat_rate'] = (market_analysis['total_repeats'] / market_analysis['total_calls']) * 100\n",
        "\n",
        "        print(\"Comparación entre mercados:\")\n",
        "        for mercado, row in market_analysis.iterrows():\n",
        "            print(f\"{mercado}:\")\n",
        "            print(f\"  • FCR: {row['fcr_pct']:.1f}%\")\n",
        "            print(f\"  • Tasa repetidas: {row['repeat_rate']:.1f}%\")\n",
        "            print(f\"  • Total llamadas: {row['total_calls']:,}\")\n",
        "\n",
        "        # Identificar mercado con mejores/peores prácticas\n",
        "        mejor_mercado = market_analysis.loc[market_analysis['fcr_pct'].idxmax()]\n",
        "        peor_mercado = market_analysis.loc[market_analysis['fcr_pct'].idxmin()]\n",
        "\n",
        "        print(f\"\\n🏆 Mejor FCR: {mejor_mercado.name} ({mejor_mercado['fcr_pct']:.1f}%)\")\n",
        "        print(f\"⚠️  Peor FCR: {peor_mercado.name} ({peor_mercado['fcr_pct']:.1f}%)\")\n",
        "\n",
        "        # Análisis cruzado: Mercado vs Tipo de Problema\n",
        "        print(f\"\\n🔍 ANÁLISIS CRUZADO MERCADO vs PROBLEMA:\")\n",
        "        if 'problem_type_label' in df.columns:\n",
        "            cross_analysis = df.pivot_table(\n",
        "                values='total_repeats',\n",
        "                index='market',\n",
        "                columns='problem_type_label',\n",
        "                aggfunc='sum',\n",
        "                fill_value=0\n",
        "            )\n",
        "\n",
        "            print(\"Llamadas repetidas por mercado y problema:\")\n",
        "            print(cross_analysis)\n",
        "\n",
        "            # Identificar combinaciones problemáticas\n",
        "            max_val = cross_analysis.max().max()\n",
        "            max_locations = np.where(cross_analysis == max_val)\n",
        "            if len(max_locations[0]) > 0:\n",
        "                mercado_prob = cross_analysis.index[max_locations[0][0]]\n",
        "                problema_prob = cross_analysis.columns[max_locations[1][0]]\n",
        "                print(f\"\\n🎯 Combinación más problemática: {mercado_prob} + {problema_prob} ({max_val:,} llamadas repetidas)\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 6. VALIDACIÓN CONTRA TABLEAU\n",
        "    # ==========================================\n",
        "\n",
        "    print(f\"\\n✅ VALIDACIÓN FINAL:\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"¿Coincide con Tableau?\")\n",
        "    print(f\"Total llamadas: {total_all_calls:,} (esperado: ~63,156)\")\n",
        "    print(f\"FCR: {fcr_rate:.1f}% (esperado: ~78.7%)\")\n",
        "    print(f\"Llamadas repetidas: {total_repeat_calls:,} (esperado: ~11,584)\")\n",
        "    print(f\"Ratio recontacto: {repeat_rate:.1f}% (esperado: ~21.3%)\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 7. RETORNAR DATOS CORREGIDOS\n",
        "    # ==========================================\n",
        "\n",
        "    return {\n",
        "        'df_corregido': df,\n",
        "        'metricas_principales': {\n",
        "            'total_calls': total_all_calls,\n",
        "            'fcr_rate': fcr_rate,\n",
        "            'repeat_rate': repeat_rate,\n",
        "            'total_repeats': total_repeat_calls\n",
        "        },\n",
        "        'escalamiento': escalamiento_df,\n",
        "        'por_problema': problem_analysis if 'problem_type' in df.columns else None,\n",
        "        'por_mercado': market_analysis if 'market' in df.columns else None\n",
        "    }\n",
        "\n",
        "def generate_escalamiento_viz(escalamiento_df, output_path='escalamiento_plot.png'):\n",
        "    \"\"\"\n",
        "    Crear visualización del patrón de escalamiento N1→N7\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Gráfico de línea para mostrar el patrón\n",
        "    plt.plot(escalamiento_df['Contacto'], escalamiento_df['Llamadas'],\n",
        "             marker='o', linewidth=2, markersize=8)\n",
        "\n",
        "    plt.title('Patrón de Escalamiento: Llamadas Repetidas por Contacto', fontsize=14)\n",
        "    plt.xlabel('Número de Contacto', fontsize=12)\n",
        "    plt.ylabel('Número de Llamadas', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Anotar valores\n",
        "    for i, row in escalamiento_df.iterrows():\n",
        "        plt.annotate(f\"{row['Llamadas']:,}\",\n",
        "                    (row['Contacto'], row['Llamadas']),\n",
        "                    textcoords=\"offset points\",\n",
        "                    xytext=(0,10),\n",
        "                    ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"📊 Gráfico guardado en: {output_path}\")\n",
        "\n",
        "# FUNCIÓN PRINCIPAL PARA EJECUTAR TODO\n",
        "def run_complete_analysis(df):\n",
        "    \"\"\"\n",
        "    Ejecutar análisis completo corregido\n",
        "    \"\"\"\n",
        "    print(\"🚀 INICIANDO ANÁLISIS COMPLETO CORREGIDO...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    results = complete_corrected_analysis(df)\n",
        "\n",
        "    # Generar visualización del escalamiento\n",
        "    if results['escalamiento'] is not None and not results['escalamiento'].empty:\n",
        "        generate_escalamiento_viz(results['escalamiento'])\n",
        "\n",
        "    return results\n",
        "\n",
        "# ==========================================\n",
        "# EJECUCIÓN AUTOMÁTICA COMPLETA\n",
        "# ==========================================\n",
        "\n",
        "def clean_and_analyze_google_fiber():\n",
        "    \"\"\"\n",
        "    Función principal que ejecuta TODO el proceso automáticamente\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Cargar y limpiar datos\n",
        "    print(\"📊 PASO 1: CARGANDO DATOS...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    url_market1 = 'https://docs.google.com/spreadsheets/d/1T7irtn0ay9MfuhG_6y2jEbeT-zLuEQTbv1qNxkpBaTE/export?format=csv'\n",
        "    url_market2 = 'https://docs.google.com/spreadsheets/d/1RE_MAKPt0JfWijbYCR32XFra9OGlmS2tLMxNs-X2NSU/export?format=csv'\n",
        "\n",
        "    try:\n",
        "        df1 = pd.read_csv(url_market1)\n",
        "        df2 = pd.read_csv(url_market2)\n",
        "        print(f\"✅ Market 1: {len(df1)} registros\")\n",
        "        print(f\"✅ Market 2: {len(df2)} registros\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error cargando datos: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 2. Estandarizar columnas\n",
        "    print(\"\\n🔧 PASO 2: ESTANDARIZANDO DATOS...\")\n",
        "    df1.columns = df1.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "    df2.columns = df2.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "    # 3. Unir datasets\n",
        "    df = pd.concat([df1, df2], ignore_index=True)\n",
        "    print(f\"Dataset combinado: {len(df)} registros\")\n",
        "\n",
        "    # 4. Limpiar fechas\n",
        "    print(\"\\n📅 PASO 3: PROCESANDO FECHAS...\")\n",
        "    if 'date_created' in df.columns:\n",
        "        df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "        invalid_dates = df['date_created'].isnull().sum()\n",
        "        if invalid_dates > 0:\n",
        "            print(f\"⚠️ Eliminando {invalid_dates} registros con fechas inválidas\")\n",
        "            df = df.dropna(subset=['date_created'])\n",
        "\n",
        "    # 5. Renombrar columnas principales\n",
        "    rename_mapping = {\n",
        "        'new_type': 'problem_type',\n",
        "        'new_market': 'market'\n",
        "    }\n",
        "    df = df.rename(columns=rename_mapping)\n",
        "\n",
        "    # 6. EJECUTAR ANÁLISIS COMPLETO\n",
        "    print(\"\\n🚀 PASO 4: EJECUTANDO ANÁLISIS COMPLETO...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    results = complete_corrected_analysis(df)\n",
        "\n",
        "    # 7. Mostrar resumen final\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎯 RESUMEN EJECUTIVO FINAL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    metrics = results['metricas_principales']\n",
        "    print(f\"📊 MÉTRICAS VALIDADAS CONTRA TABLEAU:\")\n",
        "    print(f\"   ├─ Total Llamadas: {metrics['total_calls']:,}\")\n",
        "    print(f\"   ├─ FCR Rate: {metrics['fcr_rate']:.1f}%\")\n",
        "    print(f\"   ├─ Llamadas Repetidas: {metrics['total_repeats']:,}\")\n",
        "    print(f\"   └─ Ratio Recontacto: {metrics['repeat_rate']:.1f}%\")\n",
        "\n",
        "    if results['por_problema'] is not None:\n",
        "        print(f\"\\n🥇 PROBLEMA MÁS CRÍTICO:\")\n",
        "        worst_problem = results['por_problema'].iloc[0]\n",
        "        print(f\"   └─ {worst_problem.name}: {worst_problem['repeat_rate']:.1f}% tasa repetidas\")\n",
        "\n",
        "    if results['escalamiento'] is not None:\n",
        "        print(f\"\\n📈 PATRÓN DE ESCALAMIENTO:\")\n",
        "        for _, row in results['escalamiento'].iterrows():\n",
        "            print(f\"   ├─ {row['Contacto']}: {row['Llamadas']:,} llamadas ({row['Porcentaje']:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n✅ ANÁLISIS COMPLETADO EXITOSAMENTE!\")\n",
        "\n",
        "    return df, results\n",
        "\n",
        "# ==========================================\n",
        "# EJECUCIÓN INMEDIATA AL CARGAR SCRIPT\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🎬 EJECUTANDO ANÁLISIS COMPLETO DE GOOGLE FIBER...\")\n",
        "    print(\"🔄 Procesando datos automáticamente...\\n\")\n",
        "\n",
        "    # EJECUTAR TODO AUTOMÁTICAMENTE\n",
        "    df_final, analysis_results = clean_and_analyze_google_fiber()\n",
        "\n",
        "    if df_final is not None:\n",
        "        print(\"\\n💾 GUARDANDO RESULTADOS...\")\n",
        "        df_final.to_csv('google_fiber_analysis_complete.csv', index=False)\n",
        "        print(\"📁 Archivo guardado: google_fiber_analysis_complete.csv\")\n",
        "\n",
        "        print(\"\\n🎉 ¡PROCESO COMPLETADO!\")\n",
        "        print(\"🔍 Revisa los resultados impresos arriba para validar contra Tableau\")\n",
        "    else:\n",
        "        print(\"❌ Error en el procesamiento\")\n",
        "\n",
        "# TAMBIÉN PUEDES EJECUTAR MANUALMENTE ASÍ:\n",
        "# df, results = clean_and_analyze_google_fiber()\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🚀 EJECUTANDO AUTOMÁTICAMENTE...\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtsXgj0K7E-v",
        "outputId": "93f8e401-c443-4898-fea1-ec4726435494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎬 EJECUTANDO ANÁLISIS COMPLETO DE GOOGLE FIBER...\n",
            "🔄 Procesando datos automáticamente...\n",
            "\n",
            "📊 PASO 1: CARGANDO DATOS...\n",
            "==================================================\n",
            "✅ Market 1: 450 registros\n",
            "✅ Market 2: 450 registros\n",
            "\n",
            "🔧 PASO 2: ESTANDARIZANDO DATOS...\n",
            "Dataset combinado: 900 registros\n",
            "\n",
            "📅 PASO 3: PROCESANDO FECHAS...\n",
            "\n",
            "🚀 PASO 4: EJECUTANDO ANÁLISIS COMPLETO...\n",
            "==================================================\n",
            "🔧 CORRIGIENDO TODAS LAS MÉTRICAS...\n",
            "==================================================\n",
            "📞 Columna primera llamada: contacts_n\n",
            "🔁 Columnas repetidas: ['contacts_n_1', 'contacts_n_2', 'contacts_n_3', 'contacts_n_4', 'contacts_n_5', 'contacts_n_6', 'contacts_n_7']\n",
            "\n",
            "📊 MÉTRICAS PRINCIPALES (CORREGIDAS):\n",
            "----------------------------------------\n",
            "Total de llamadas: 63,156.0\n",
            "├─ Primera llamada: 49,722.0\n",
            "└─ Llamadas repetidas: 13,434.0\n",
            "FCR Rate: 78.7%\n",
            "Repeat Rate: 21.3%\n",
            "Casos totales: 900\n",
            "├─ Sin repetición: 252 (28.0%)\n",
            "└─ Con repetición: 648 (72.0%)\n",
            "\n",
            "🔍 ANÁLISIS DE ESCALAMIENTO POR CONTACTO:\n",
            "---------------------------------------------\n",
            "Contacto N1: 3,651.0 llamadas (27.2%)\n",
            "Contacto N2: 2,295.0 llamadas (17.1%)\n",
            "Contacto N3: 1,780.0 llamadas (13.2%)\n",
            "Contacto N4: 1,558.0 llamadas (11.6%)\n",
            "Contacto N5: 1,492.0 llamadas (11.1%)\n",
            "Contacto N6: 1,315.0 llamadas (9.8%)\n",
            "Contacto N7: 1,343.0 llamadas (10.0%)\n",
            "\n",
            "🎯 Pico máximo en: N1 con 3,651.0 llamadas\n",
            "⚠️  Aumentos detectados en: N7 (+2.1% vs N6)\n",
            "\n",
            "📋 ANÁLISIS POR TIPO DE PROBLEMA:\n",
            "-----------------------------------\n",
            "\n",
            "Ranking por Mayor Tasa de Repetición:\n",
            "Scheduling:\n",
            "  • Llamadas repetidas: 715.0 (33.3%)\n",
            "  • FCR: 66.7%\n",
            "  • Promedio llamadas/caso: 11.9\n",
            "Construction:\n",
            "  • Llamadas repetidas: 103.0 (25.3%)\n",
            "  • FCR: 74.7%\n",
            "  • Promedio llamadas/caso: 2.3\n",
            "Internet/WiFi:\n",
            "  • Llamadas repetidas: 6,365.0 (24.6%)\n",
            "  • FCR: 75.4%\n",
            "  • Promedio llamadas/caso: 143.9\n",
            "Account Manager:\n",
            "  • Llamadas repetidas: 902.0 (24.4%)\n",
            "  • FCR: 75.6%\n",
            "  • Promedio llamadas/caso: 20.5\n",
            "Technical:\n",
            "  • Llamadas repetidas: 5,349.0 (17.2%)\n",
            "  • FCR: 82.8%\n",
            "  • Promedio llamadas/caso: 172.3\n",
            "\n",
            "🔍 ESCALAMIENTO POR TIPO DE PROBLEMA:\n",
            "\n",
            "🏙️ ANÁLISIS POR MERCADO:\n",
            "-------------------------\n",
            "Comparación entre mercados:\n",
            "market_1:\n",
            "  • FCR: 78.2%\n",
            "  • Tasa repetidas: 21.8%\n",
            "  • Total llamadas: 57,980.0\n",
            "market_2:\n",
            "  • FCR: 84.8%\n",
            "  • Tasa repetidas: 15.2%\n",
            "  • Total llamadas: 5,176.0\n",
            "\n",
            "🏆 Mejor FCR: market_2 (84.8%)\n",
            "⚠️  Peor FCR: market_1 (78.2%)\n",
            "\n",
            "🔍 ANÁLISIS CRUZADO MERCADO vs PROBLEMA:\n",
            "Llamadas repetidas por mercado y problema:\n",
            "problem_type_label  Account Manager  Construction  Internet/WiFi  Scheduling  \\\n",
            "market                                                                         \n",
            "market_1                      850.0          95.0         5969.0       691.0   \n",
            "market_2                       52.0           8.0          396.0        24.0   \n",
            "\n",
            "problem_type_label  Technical  \n",
            "market                         \n",
            "market_1               5042.0  \n",
            "market_2                307.0  \n",
            "\n",
            "🎯 Combinación más problemática: market_1 + Internet/WiFi (5,969.0 llamadas repetidas)\n",
            "\n",
            "✅ VALIDACIÓN FINAL:\n",
            "--------------------\n",
            "¿Coincide con Tableau?\n",
            "Total llamadas: 63,156.0 (esperado: ~63,156)\n",
            "FCR: 78.7% (esperado: ~78.7%)\n",
            "Llamadas repetidas: 13,434.0 (esperado: ~11,584)\n",
            "Ratio recontacto: 21.3% (esperado: ~21.3%)\n",
            "\n",
            "============================================================\n",
            "🎯 RESUMEN EJECUTIVO FINAL\n",
            "============================================================\n",
            "📊 MÉTRICAS VALIDADAS CONTRA TABLEAU:\n",
            "   ├─ Total Llamadas: 63,156.0\n",
            "   ├─ FCR Rate: 78.7%\n",
            "   ├─ Llamadas Repetidas: 13,434.0\n",
            "   └─ Ratio Recontacto: 21.3%\n",
            "\n",
            "🥇 PROBLEMA MÁS CRÍTICO:\n",
            "   └─ Scheduling: 33.3% tasa repetidas\n",
            "\n",
            "📈 PATRÓN DE ESCALAMIENTO:\n",
            "   ├─ N1: 3,651.0 llamadas (27.2%)\n",
            "   ├─ N2: 2,295.0 llamadas (17.1%)\n",
            "   ├─ N3: 1,780.0 llamadas (13.2%)\n",
            "   ├─ N4: 1,558.0 llamadas (11.6%)\n",
            "   ├─ N5: 1,492.0 llamadas (11.1%)\n",
            "   ├─ N6: 1,315.0 llamadas (9.8%)\n",
            "   ├─ N7: 1,343.0 llamadas (10.0%)\n",
            "\n",
            "✅ ANÁLISIS COMPLETADO EXITOSAMENTE!\n",
            "\n",
            "💾 GUARDANDO RESULTADOS...\n",
            "📁 Archivo guardado: google_fiber_analysis_complete.csv\n",
            "\n",
            "🎉 ¡PROCESO COMPLETADO!\n",
            "🔍 Revisa los resultados impresos arriba para validar contra Tableau\n",
            "\n",
            "============================================================\n",
            "🚀 EJECUTANDO AUTOMÁTICAMENTE...\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df.to_csv('/content/drive/MyDrive/google_fiber_clean.csv', index=False)\n"
      ],
      "metadata": {
        "id": "PR-yqTU7RL08",
        "outputId": "b80aad48-9f3b-4d60-abe5-2b7e7a89c1fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. OPCION SIMPLE DE LIMPIEZA DE DATOS"
      ],
      "metadata": {
        "id": "_obsfZ9MM9RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Cargar datos desde Google Sheets como CSV\n",
        "url_market1 = 'https://docs.google.com/spreadsheets/d/1T7irtn0ay9MfuhG_6y2jEbeT-zLuEQTbv1qNxkpBaTE/export?format=csv'\n",
        "url_market2 = 'https://docs.google.com/spreadsheets/d/1RE_MAKPt0JfWijbYCR32XFra9OGlmS2tLMxNs-X2NSU/export?format=csv'\n",
        "\n",
        "df1 = pd.read_csv(url_market1)\n",
        "df2 = pd.read_csv(url_market2)\n",
        "\n",
        "# 2. Estandarizar nombres de columnas\n",
        "df1.columns = df1.columns.str.strip().str.lower()\n",
        "df2.columns = df2.columns.str.strip().str.lower()\n",
        "\n",
        "# 3. Verificar estructura común\n",
        "assert set(df1.columns) == set(df2.columns), \"⚠️ Columnas no coinciden entre datasets\"\n",
        "\n",
        "# 4. Unir los dataframes\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# 5. Eliminar duplicados\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# 6. Verificar y convertir fecha\n",
        "df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
        "\n",
        "# 7. Eliminar filas con fechas nulas o contactos vacíos\n",
        "df = df.dropna(subset=['date_created', 'contacts_n'])\n",
        "\n",
        "# 8. Convertir columnas numéricas\n",
        "contact_cols = ['contacts_n', 'contacts_n_1', 'contacts_n_2', 'contacts_n_3',\n",
        "                'contacts_n_4', 'contacts_n_5', 'contacts_n_6', 'contacts_n_7']\n",
        "\n",
        "df[contact_cols] = df[contact_cols].fillna(0).astype(int)\n",
        "\n",
        "# 9. Renombrar columnas para claridad\n",
        "df = df.rename(columns={\n",
        "    'new_type': 'problem_type',\n",
        "    'new_market': 'market'\n",
        "})\n",
        "\n",
        "# 10. Calcular columnas clave\n",
        "repeat_cols = ['contacts_n_1', 'contacts_n_2', 'contacts_n_3',\n",
        "               'contacts_n_4', 'contacts_n_5', 'contacts_n_6', 'contacts_n_7']\n",
        "\n",
        "df['total_repeats'] = df[repeat_cols].sum(axis=1)\n",
        "df['total_calls'] = df['contacts_n'] + df['total_repeats']\n",
        "df['resolved_first_call'] = (df['total_repeats'] == 0).astype(int)  # 1 = resuelto en primera llamada\n",
        "\n",
        "# 11. Crear columna de semana y mes para análisis temporal\n",
        "df['week'] = df['date_created'].dt.to_period('W').astype(str)\n",
        "df['month'] = df['date_created'].dt.to_period('M').astype(str)\n",
        "\n",
        "# 12. Revisar tipos y valores únicos\n",
        "print(\"✅ Datos limpios y listos. Vista previa:\")\n",
        "print(df.dtypes)\n",
        "print(df.head())\n",
        "\n",
        "# 13. Exportar CSV limpio\n",
        "df.to_csv('google_fiber_llamadas_limpias.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYDFu55nzeEu",
        "outputId": "8dfcaa7e-b515-49e7-837b-8d2dd79c2f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Datos limpios y listos. Vista previa:\n",
            "date_created           datetime64[ns]\n",
            "contacts_n                      int64\n",
            "contacts_n_1                    int64\n",
            "contacts_n_2                    int64\n",
            "contacts_n_3                    int64\n",
            "contacts_n_4                    int64\n",
            "contacts_n_5                    int64\n",
            "contacts_n_6                    int64\n",
            "contacts_n_7                    int64\n",
            "problem_type                   object\n",
            "market                         object\n",
            "total_repeats                   int64\n",
            "total_calls                     int64\n",
            "resolved_first_call             int64\n",
            "week                           object\n",
            "month                          object\n",
            "dtype: object\n",
            "  date_created  contacts_n  contacts_n_1  contacts_n_2  contacts_n_3  \\\n",
            "0   2022-02-04         199            21             6            11   \n",
            "1   2022-01-30          19             2             0             2   \n",
            "2   2022-02-14          29             0             2             2   \n",
            "3   2022-01-16         120             6             6             5   \n",
            "4   2022-02-03         182            27            13             0   \n",
            "\n",
            "   contacts_n_4  contacts_n_5  contacts_n_6  contacts_n_7 problem_type  \\\n",
            "0             7            14             5             6       type_5   \n",
            "1             1             0             0             0       type_1   \n",
            "2             0             1             0             1       type_1   \n",
            "3             4             7             4             0       type_2   \n",
            "4            14             4             3             2       type_5   \n",
            "\n",
            "     market  total_repeats  total_calls  resolved_first_call  \\\n",
            "0  market_1             70          269                    0   \n",
            "1  market_1              5           24                    0   \n",
            "2  market_1              6           35                    0   \n",
            "3  market_1             32          152                    0   \n",
            "4  market_1             63          245                    0   \n",
            "\n",
            "                    week    month  \n",
            "0  2022-01-31/2022-02-06  2022-02  \n",
            "1  2022-01-24/2022-01-30  2022-01  \n",
            "2  2022-02-14/2022-02-20  2022-02  \n",
            "3  2022-01-10/2022-01-16  2022-01  \n",
            "4  2022-01-31/2022-02-06  2022-02  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similitudes y diferencias en los 2 analisis"
      ],
      "metadata": {
        "id": "YQ2Ry7OnOZ-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Similitudes\n",
        "\n",
        "Ambos:\n",
        "\n",
        "Cargan los dos datasets desde Google Sheets.\n",
        "\n",
        "Estandarizan nombres de columnas.\n",
        "\n",
        "Unifican los datasets en un solo DataFrame.\n",
        "\n",
        "Limpian duplicados y nulos básicos.\n",
        "\n",
        "Procesan fechas (date_created).\n",
        "\n",
        "Normalizan columnas de contactos a enteros (contacts_n, contacts_n_1...).\n",
        "\n",
        "Calculan métricas claves:\n",
        "\n",
        "total_repeats\n",
        "\n",
        "total_calls\n",
        "\n",
        "resolved_first_call\n",
        "\n",
        "Crean variables temporales (week, month en la simple; year, month, week, day_of_week en la avanzada).\n",
        "\n",
        "Exportan el resultado como CSV limpio.\n",
        "\n",
        "🔹 Diferencias clave\n",
        "Aspecto\tVersión simple\tVersión mejorada\n",
        "Carga de datos\tUsa pd.read_csv directamente, sin validación.\tManeja errores con try/except y muestra el conteo de registros por mercado.\n",
        "Columnas\tassert para que ambas tablas tengan las mismas columnas. Si no coinciden, da error.\tMás flexible: muestra diferencias y trabaja solo con columnas comunes.\n",
        "Fechas\tConvierte date_created con un único intento (pd.to_datetime).\tIntenta múltiples formatos (%Y-%m-%d, %d/%m/%Y, etc.), reporta conversiones y elimina inválidas.\n",
        "Duplicados\tSolo elimina filas duplicadas completas.\tDetecta duplicados por fecha, mercado y tipo de problema, y los agrega sumando contactos. Mucho más realista.\n",
        "Contactos\tAsume columnas fijas (contacts_n, contacts_n_1...contacts_n_7).\tBusca dinámicamente todas las columnas que contengan \"contacts_\", escalable si el dataset cambia.\n",
        "Categóricas\tSolo renombra new_type → problem_type y new_market → market.\tLimpia espacios, reemplaza nan, null, None y reporta los valores únicos para revisión.\n",
        "Métricas\tCalcula resolved_first_call (binario).\tAdemás:\n",
        "\n",
        "repeat_call_flag (llamadas repetidas)\n",
        "\n",
        "FCR rate (%)\n",
        "\n",
        "Repeat Call Rate (%) |\n",
        "| Variables temporales | week y month. | Más completas: year, month, week, day_of_week, date_only, year_month, year_week. |\n",
        "| Validación | Solo imprime dtypes y head(). | Hace análisis completo: rango de fechas, nulos restantes, estadísticas descriptivas. |\n",
        "| Exportación | Exporta un CSV. | Exporta CSV + genera un reporte de calidad en TXT con distribución por mercado, tipo de problema, métricas de rendimiento y columnas disponibles. |\n",
        "| Escalabilidad | Adecuada para datasets simples y controlados. | Robusta y adaptable a datasets heterogéneos y en producción. |"
      ],
      "metadata": {
        "id": "FdlLL6p9OX02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":: # *json promp para otras limpiezas*:"
      ],
      "metadata": {
        "id": "3JNMuzzwPKuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"data_cleaning_steps\": [\n",
        "    {\n",
        "      \"step\": 1,\n",
        "      \"action\": \"load_data\",\n",
        "      \"details\": {\n",
        "        \"source\": [\"csv\", \"excel\", \"sql\", \"google_sheets\"],\n",
        "        \"error_handling\": \"try/except\",\n",
        "        \"log\": [\"rows\", \"columns\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 2,\n",
        "      \"action\": \"standardize_columns\",\n",
        "      \"details\": {\n",
        "        \"lowercase\": true,\n",
        "        \"strip_spaces\": true,\n",
        "        \"replace_spaces_with\": \"_\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 3,\n",
        "      \"action\": \"check_structure\",\n",
        "      \"details\": {\n",
        "        \"validate_columns\": true,\n",
        "        \"handle_differences\": [\"report\", \"keep_common\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 4,\n",
        "      \"action\": \"merge_datasets\",\n",
        "      \"details\": {\n",
        "        \"method\": \"concat\",\n",
        "        \"ignore_index\": true\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 5,\n",
        "      \"action\": \"analyze_nulls\",\n",
        "      \"details\": {\n",
        "        \"metrics\": [\"count\", \"percentage\"],\n",
        "        \"strategy\": [\"drop\", \"impute\", \"keep\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 6,\n",
        "      \"action\": \"process_dates\",\n",
        "      \"details\": {\n",
        "        \"formats\": [\"%Y-%m-%d\", \"%d/%m/%Y\", \"%m/%d/%Y\"],\n",
        "        \"convert_to\": \"datetime\",\n",
        "        \"new_features\": [\"year\", \"month\", \"week\", \"day_of_week\", \"periods\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 7,\n",
        "      \"action\": \"process_numeric\",\n",
        "      \"details\": {\n",
        "        \"convert\": \"pd.to_numeric\",\n",
        "        \"fillna\": 0,\n",
        "        \"types\": [\"int\", \"float\"],\n",
        "        \"outliers\": \"optional\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 8,\n",
        "      \"action\": \"process_categorical\",\n",
        "      \"details\": {\n",
        "        \"clean_spaces\": true,\n",
        "        \"lowercase\": true,\n",
        "        \"replace_invalid\": [\"NaN\", \"null\", \"None\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 9,\n",
        "      \"action\": \"handle_duplicates\",\n",
        "      \"details\": {\n",
        "        \"subset\": [\"id\", \"date\", \"category\"],\n",
        "        \"strategy\": [\"drop\", \"aggregate\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 10,\n",
        "      \"action\": \"generate_metrics\",\n",
        "      \"details\": {\n",
        "        \"totals\": [\"sum_columns\"],\n",
        "        \"flags\": [\"binary_indicators\"],\n",
        "        \"ratios\": [\"conversion_rate\", \"repeat_rate\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 11,\n",
        "      \"action\": \"final_validation\",\n",
        "      \"details\": {\n",
        "        \"check_nulls\": true,\n",
        "        \"check_date_range\": true,\n",
        "        \"summary\": [\"df.info\", \"df.describe\"]\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 12,\n",
        "      \"action\": \"export_data\",\n",
        "      \"details\": {\n",
        "        \"format\": [\"csv\", \"excel\", \"db\"],\n",
        "        \"filename\": \"dataset_clean_DATE.csv\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"step\": 13,\n",
        "      \"action\": \"generate_report\",\n",
        "      \"details\": {\n",
        "        \"include\": [\n",
        "          \"initial_vs_final_records\",\n",
        "          \"null_handling_summary\",\n",
        "          \"categorical_distribution\",\n",
        "          \"key_metrics\"\n",
        "        ],\n",
        "        \"output\": \"txt\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "4qGKtfrWPJuz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}